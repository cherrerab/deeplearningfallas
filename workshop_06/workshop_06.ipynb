{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw6EXItut1vF"
      },
      "source": [
        "# C-MAPSS Anomaly Detection Autoencoder\n",
        "\n",
        "La detección de anomalías (Anomaly Detection) sobre sistemas con una alta dimensionalidad de datos, es un problema de particular interés tanto en el campo del Machine Learning como también en diversas áreas de la ingeniería e industria. En particular, los algoritmos de detección de anomalías resultan de suma relevancia a la hora de desarrollar e implementar sistemas de monitoreo de equipos o activos industriales de mayor complejidad.\n",
        "\n",
        "En términos generales, un algoritmo de detección de anomalías cumple la función de reconocer patrones o colecciones de datos que escapan del estado nominal del sistema analizado. Estadísticamente, estos algoritmos generar un mapeo multidimensional de probabilidad, donde las anomalías consisten en los puntos `outliers` que se encuentran en áreas con una baja densidad de probabilidad respecto al estado de operación nominal.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_05/bin/nasa_logo.png\" height=\"200\"><img src=\"https://www.researchgate.net/profile/Xiaolei_Fang/publication/310789705/figure/fig2/AS:462413100261377@1487259282977/Simplified-diagram-of-engine-simulated-in-C-MAPSS-6.png\" height=\"200\">\n",
        "\n",
        "En este workshop volveremos a utilizar el C-MAPSS Dataset de la NASA para desarrollar un detector de anomalías mediante un Autoencoder. Así, en este caso reconfiguraremos los datos de las simulaciones ya no para predecir el tiempo de vida remanente o Remaining Useful Life (RUL) de cada turbina, sino para clasificar si la turbina se encuentra en un estado de operación nominal o en uno de degradación crítica.\n",
        "\n",
        "El `autoencoder` que implementaremos mediante `keras` y `tensorflow` tendrá la finalidad de reconstruir, mediante un modelo `encoder` y otro `decoder`, la serie temporal del registro de los 14 sensores del sistema. De esta manera, al entrenar el `autoencoder` únicamente sobre las secuencias correspondientes a un estado `nominal` de operación, se espera que al ingresar series temporales en un estado `degradado` el `autoencoder` presente un desempeño considerablemente menor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-R8SDBLGuBS"
      },
      "source": [
        "## C-MAPSS Dataset\n",
        "\n",
        "Como recordatorio, el C-MAPSS dataset que utilizaremos en esta ocación consiste en múltiples simulaciones de degradación de motores turbofán bajo distintas condiciones, tanto mecánicas como operacionales. En este sentido, el dataset se compone de más de `27000` series temporales de los sensores del turbofán que, gracias a la simulación de este, han podido ser asociadas al RUL del motor.\n",
        "\n",
        "En esta sección nos encargaremos de cargar y explorar el dataset que utilizaremos, denominado `C-MAPSS FD001`, que contiene `100` simulaciones de motores mecánicamente distintos bajo condiciones nominales de operación. Nuevamente, para facilitar la carga del dataset a este entorno de Google Colab el achivo `CMAPSS_FD001.npz` ha sido cargado a un Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoNStL9XtzYl"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# inicializar GoogleDrive con credenciales de autorización\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# crear carpeta para descargar los archivos .npz\n",
        "!mkdir /content/datasets\n",
        "\n",
        "# Google Drive IDs para descargar los archivos .npz\n",
        "files_id = [('CMAPSS_FD001.npz', '1DNJFrQKB4I7SqjpFmM5SsKRaN7O9XCNZ')]\n",
        "\n",
        "# comenzar descarga\n",
        "print('descargando datasets: ', end='')\n",
        "\n",
        "for filename, id in files_id:\n",
        "  save_path = os.path.join('/content/datasets', filename)\n",
        "\n",
        "  # descargar y guardar en /content/datasets\n",
        "  downloaded = drive.CreateFile({'id': id}) \n",
        "  downloaded.GetContentFile(save_path)\n",
        "\n",
        "# indicar descarga terminada\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DYsx7d0J7W5"
      },
      "source": [
        "Carguemos este archivo mediante `np.load()` y exploremos las estructuras y datos que contiene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyyrU9ekKkR5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# cargar archivo CMAPSS_FD001.npz\n",
        "dataset = np.load('/content/datasets/CMAPSS_FD001.npz', allow_pickle=True)\n",
        "\n",
        "# print keys del dataset\n",
        "print(  list( dataset.keys() ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3HlYqCndIHq"
      },
      "source": [
        "También, como ya es rutinario, aprovecharemos de cargar el GitHub del curso deeplearningfallas para disponer de algunas utilidades que nos serán de utilidad más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8PvO5qedOiq"
      },
      "source": [
        "# cargar GitHub https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/\n",
        "!git clone https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/deeplearningfallas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmfqu8zbLAm5"
      },
      "source": [
        "---\n",
        "# Training Data\n",
        "\n",
        "Como ya sabemos, el dataset se encuentra previamente separado en conjuntos de `training` y `testing`. En particular, cada uno de estos conjuntos están separados en `100` simulaciones de turbinas distintas. Luego, cada una de estas simulaciones contiene varias muestras de series temporales registradas durante la operación que están asociadas a un valor RUL en el conjunto `Y_train` o `Y_test`.\n",
        "\n",
        "Nuevamente, para generar los conjuntos `training` y `testing` que finalmente utilizaremos para desarrollar nuestros modelos, debemos concatenar las series de todas las simulaciones de `C-MAPSS` y modificar sus dimensiones para que sean compatibles con las capas y estructuras de `tensorflow` y `keras`. En particular, recordemos que los datos de cada simulación en el conjunto `X_train` son de la forma `(n_samples, n_timesteps, n_features, 1)`. Donde en este caso `n_features` corresponde a la cantidad de sensores dentro de la simulación (`14`), mientras `n_timesteps` corresponde a la cantidad de puntos temporales o mediciones dentro de la serie (`30`). Por otro lado, los datos del conjunto `Y_train` son de la forma `(n_samples, 1)`, donde cada valor corresponde al RUL de las respectivas series temporales en el conjunto `X_train`.\n",
        "\n",
        "En este caso, a diferencia del workshop anterior, aplanaremos las series temporales de cada sample en un único vector de la forma `(n_timesteps*n_features, )`, pues consideraremos cada punto dentro del registro temporal como una dimensión que define el espacio vectorial de nuestra colección de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89uZwVIvF9ci"
      },
      "source": [
        "# extraer conjuntos de training y testing del dataset\n",
        "X_train, Y_train = dataset['x_train'], dataset['y_train']\n",
        "X_test, Y_test = dataset['x_test'], dataset['y_test']\n",
        "\n",
        "# ---\n",
        "# concatenar y reconfigurar datos de las 100 simulaciones CMAPSS\n",
        "# en este caso reordenaremos cada sample de la forma (n_timesteps*n_features)\n",
        "# para ingresarlos al autoencoder que desarrollaremos.\n",
        "\n",
        "# los datos X deben ser de la forma (n_samples, n_timesteps*n_features)\n",
        "X_train = \n",
        "X_train = \n",
        "\n",
        "X_test = \n",
        "X_test = \n",
        "\n",
        "# los datos Y deben ser de la forma (n_samples, 1)\n",
        "Y_train = \n",
        "Y_train = \n",
        "\n",
        "Y_test = \n",
        "Y_test = \n",
        "\n",
        "# print dimensiones de los nuevos conjuntos\n",
        "print('X_train.shape: ', X_train.shape)\n",
        "print('Y_train.shape: ', Y_train.shape)\n",
        "print('\\nX_test.shape: ', X_test.shape)\n",
        "print('Y_test.shape: ', Y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqFmTiRNKv9W"
      },
      "source": [
        "Ahora, como se mencionó en la introducción de este workshop, para el entrenamiento del `autoencoder` que desarrollaremos utilizaremos únicamente datos o `samples` que presenten una condición `nominal` de operación. Arbitrariamente definirimos esta condición en función del RUL de cada sample, donde aquellos que presenten un RUL menor a 0.3, se encontrarán en un estado de degradación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqQr_I-OKt3A"
      },
      "source": [
        "# ---\n",
        "# segementar samples que presenten una serie temporal de operación 'nominal'\n",
        "# en este caso, un sample 'nominal' es aquel cuyo RUL > 0.3\n",
        "\n",
        "# obtener indices nominal_idx de samples nominales\n",
        "# mediante np.where\n",
        "RUL_train = \n",
        "nominal_idx = np.where(  )\n",
        "\n",
        "# segementar X_train con samples nominales\n",
        "X_train_nominal = \n",
        "\n",
        "# print dimensiones de este nuevo conjunto\n",
        "print('X_train_nominal.shape: ', X_train_nominal.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo1_Q6W0VZ1l"
      },
      "source": [
        "Como tenemos una gran cantidad de datos tanto en el conjunto de `training`, como en el de `testing`, siguiendo el procedimiento apropiado para el desarrollo de modelos de deep learning, utilizaremos la función `sklearn.train_test_split` para generar un conjunto de `validation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68RMVgNAmyLo",
        "outputId": "a3323c18-0514-4849-e72b-541bc8c67e0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---\n",
        "# realizar data splitting para generar los conjuntos (X_train_AE, X_val_AE)\n",
        "# para el entrenamiento del autoencoder.\n",
        "\n",
        "# dado que los autoencoders son modelos de reeconstrucción, en este caso\n",
        "# no es necesario extraer conjuntos Y.\n",
        "X_train_AE, X_val_AE, _, _ = train_test_split(, ,\n",
        "                                              train_size= , test_size= ,\n",
        "                                              random_state= )\n",
        "\n",
        "# print sample distribution\n",
        "print( 'train split: {:d} samples'.format(X_train_AE.shape[0]) )\n",
        "print( '\\nvalidation split: {:d} samples'.format(X_val_AE.shape[0]) )"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train split: 9269 samples\n",
            "\n",
            "validation split: 2318 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFNvCKLNbKMl"
      },
      "source": [
        "---\n",
        "# Model Building\n",
        "\n",
        "Ahora comenzaremos a implementar el `autoencoder` en que se centra el trabajo de este workshop. Un `autoencoder`, en términos del deep learning, es una arquitectura de redes neuronales cuyo objetivo es reconstruir la información de entrada o bien, `input` de la red. En este sentido, un `autoencoder` perfecto es aquel que es capaz de reconstruir perfectamente los datos de entrada sin ninguna pérdida de información.\n",
        "\n",
        "Pero entonces, si los `autoencoders` solo retornan la misma información que uno ingresa al modelo, que finalidad tiene implementar un modelo así?\n",
        "\n",
        "En realidad, dentro de un `autoencoder` lo que importa no es la salida de este sino el `vector latente` que, en algún punto en su interior, concentra toda la información de los datos de entrada. Los `autoencoders` se componen de dos modelos independientes, pero que se entrenan en conjunto, el `encoder` y el `decoder`. Por un lado, el `encoder` se encarga de comprimir la información de los datos de entrada en un único vector latente cuya dimensionalidad puede ser significativamente menor a la del `input`, mientras que el `decoder` se encarga de aprender a reconstruir los datos originales que se ingresaron al `encoder`, únicamente a partir del vector latente resultante del `encoder`.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1000/0*uq2_ZipB9TqI9G_k\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfrmaXKf2DH"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "Para configurar nuestro modelo, debemos definir separadamente los modelos `encoder` y `decoder` que lo compondrán. Similar a la figura de arriba, para configurar nuestro modelo de regresión utilizaremos nuevamente la librería `keras` o `tf.keras` y se compondrá principalmente de capas `Dense`, también llamadas Fully Connected en la literatura.\n",
        "\n",
        "- https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "De esta forma, en términos generales, compondremos nuestro `encoder` de una serie de capas `Dense`, que se encargarán de procesar la información y los features de los datos de entrada hasta una última capa `Dense` cuya cantidad de nodos definirá la dimensionalidad del `vector latente`. De la misma manera, ya sea simétricamente o no, el `decoder` recibirá como entrada el `vector latente` y mediante otra serie de capas `Dense` reconstruirá los datos originales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GScDB0L5ixqX"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Reshape\n",
        "\n",
        "# en este caso crearemos el autoencoder usando la API Functional\n",
        "# la API Functional de keras permite la creación de modelos de una\n",
        "# manera mucho más flexible\n",
        "# https://keras.io/guides/functional_api/\n",
        "\n",
        "# ---\n",
        "# primero debemos crear nuestra capa Input donde debemos especificar\n",
        "# las dimensiones de los datos que se ingresarán al modelo\n",
        "# en este caso el autoencoder recibe samples de la forma (420, )\n",
        "input_dim =\n",
        "input_layer = Input( shape=input_dim )\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas Dense para configurar\n",
        "# el modelo encoder.\n",
        "# https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "encoder = Dense(units=256, activation='relu')(input_layer)\n",
        "encoder = (encoder)\n",
        "\n",
        "# ahora configuraremos la capa Dense cuyo output corresponderá al\n",
        "# vector latente del autoencoder.\n",
        "latent_dim = 64\n",
        "latent_vector = Dense(units=latent_dim, activation='relu')(encoder)\n",
        "\n",
        "# finalmente, debemos ir agregando nuestras capas Dense para configurar\n",
        "# el decoder de nuestro modelo.\n",
        "# en este caso hay que tener presente que la salida de esta sección\n",
        "# debe tener la misma dimensionalidad o forma que los samples de entrada.\n",
        "\n",
        "decoder = Dense(units= , activation= )(latent_vector)\n",
        "decoder = (decoder)\n",
        "\n",
        "output_layer = Dense(units= , activation='linear')(decoder)\n",
        "\n",
        "# ---\n",
        "# ahora configuraremos el modelo autoencoder que entrenaremos\n",
        "autoencoder = Model(input_layer, output_layer)\n",
        "\n",
        "# adicionalmente podemos configurar el modelo encoder\n",
        "# para utilizarlo como un reductor dimensional\n",
        "# esto resulta más práctico cuando la dimensión es 2 o 3\n",
        "encoder_model = Model(input_layer, latent_vector)\n",
        "\n",
        "# print model.summary()\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x50vOo4ulG6h"
      },
      "source": [
        "## Compile Model\n",
        "\n",
        "Antes de poner a entrenar al modelo, es necesario realizar unas configuraciones adicionales. En particular, debemos especificar la función de pérdida o `loss function` que se optimizará durante el entrenamiento y el método de optimización como SGD o Adam.\n",
        "- https://keras.io/api/models/model_training_apis/\n",
        "- https://keras.io/api/optimizers/\n",
        "\n",
        "En este caso, dado que la función escencial del autoencoder es reconstruir la información de entrada, configuraremos el entrenamiento para reducir el error cuadrado medio (MSE) entre los datos originales y la reconstrucción o salida del autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu0VML-llJIw"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# configurar optimizador Adam\n",
        "# https://keras.io/api/optimizers/adam/\n",
        "opt = Adam( learning_rate=1e-3 )\n",
        "\n",
        "# ---\n",
        "# compilar modelo siguiendo como función de pérdida\n",
        "# en este caso, usaremos el error cuadrádo medio (mse)\n",
        "autoencoder.compile(loss= , optimizer=opt, metrics=[ ])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUlBKDlXlZY3"
      },
      "source": [
        "## Model Training\n",
        "Hemos llegado a la parte final del proceso, para entrenar nuestro modelo debemos especificar los sets que utilizaremos para el proceso `(X_train, Y_train)`, la cantidad de `epochs` que durará el entrenamiento, y el `batch size` de muestras que se irán entregando al modelo a medida que este va iterativamente ajustando sus parámetros.\n",
        "\n",
        "En este caso, es necesario especificar que los datos `Y_train` del entrenamiento son equivalentes a los datos de entrada `X_train`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjib53pTldpq"
      },
      "source": [
        "from utils import plot_loss_function\n",
        "\n",
        "# realizar rutina de entrenamiento\n",
        "train_history = autoencoder.fit(X_train_AE, X_train_AE,\n",
        "                                batch_size=, epochs=,\n",
        "                                validation_data=( , ))\n",
        "\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(train_history, figsize=(10,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XM2Z9roeFiw"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Ahora, recordemos que la razón incial del desarrollo de este autoencoder es analizar si este puede ser utilizado para segementar o bien, permitir el reconocimiento de series temporales que presenten un estado de degradación crítico. De este modo, evaluaremos si existe una diferencia significativa entre el error de reconstrucción sobre los datos `nominales` y los `degradados`. Para emplear el `autoencoder` sobre nuevas series temporales, conviene utilizar el método `keras.Model.predict`, análogamente a como hemos hecho en las arquitecturas anteriores.\n",
        "\n",
        "Por otro lado, dado que estamos frente a un problema de regresión no podemos utilizar las herramientas de visualización anteriores como la matriz de consfusión. En este caso utilizaremos un `scatter plot` medainte la función `utils.plot_predict_scatter` para visualizar la correlación entre las predicciones `Y_pred` y los valores reales `Y_true`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z4SlI92fXcO"
      },
      "source": [
        "from utils import plot_predict_scatter\n",
        "\n",
        "# ---\n",
        "# obtener predicciones de X_test del modelo\n",
        "# en primer lugar, similar a como hicimos para X_train\n",
        "# debemos segmentar las series nominales de las degradadas.\n",
        "\n",
        "# obtener indices nominal_idx de samples nominales\n",
        "RUL_test = \n",
        "nominal_idx = np.where(  )\n",
        "degraded_idx = np.where(  )\n",
        "\n",
        "# segementar samples nominales y degradados\n",
        "X_test_nominal = X_test[nominal_idx, :]\n",
        "X_test_degraded = X_test[degraded_idx, :]\n",
        "\n",
        "# obtener recontrucciones del autoencoder\n",
        "# se puede utilizar autoencoder.predict()\n",
        "AE_test_nominal = \n",
        "AE_test_degraded = \n",
        "\n",
        "# obtener el rmse de las reconstrucciones\n",
        "rmse_nominal = np.sqrt( np.mean( np.power(X_test_nominal - AE_test_nominal, 2 ), axis=1) )\n",
        "rmse_degraded = np.sqrt( np.mean( np.power(X_test_degraded - AE_test_degraded, 2 ), axis=1) )\n",
        "\n",
        "print('nominal rmse: {:1.3f}'.format( np.mean(rmse_nominal)) )\n",
        "print('degraded rmse: {:1.3f}'.format( np.mean(rmse_degraded)) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdINL_J4pa8P"
      },
      "source": [
        "Finalmente, podemos evaluar visualmente el desempeño de nuestro detector de anomalías ploteando mediante un `scatter` plot el rmse de reconstrucción de cada uno de los samples presentes en el `X_test`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDvyGfJdhOyZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# obtener predicciones sobre todo el test set\n",
        "AE_test = \n",
        "rmse_test = \n",
        "\n",
        "# sort test samples para segmentar grupos nominales y degradados\n",
        "sample_label = np.array( RUL_test <= 0.3, dtype=int )\n",
        "sample_sort = np.argsort(sample_label)\n",
        "\n",
        "# reordenar samples: primero los samples nominales y luego los degradados\n",
        "sample_label = \n",
        "rmse_test = \n",
        "\n",
        "# scatter error de reconstrucción\n",
        "plt.figure( figsize=(15, 5) )\n",
        "plt.title('Reconstruction error over X_test')\n",
        "plt.xlabel('Sample'); plt.ylabel('Reconstruction RMSE')\n",
        "\n",
        "plt.scatter(np.arange(rmse_test.size), rmse_test,\n",
        "            c=list(sample_label), cmap='viridis', vmax=2.0, alpha=0.5, s=40)\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZQdpV2R6lcc"
      },
      "source": [
        "De esta manera, podemos ver que dentro del conjunto de series temporales degradadas existen puntos que escapan significativamente del rango de rmse. Podemos detectar o bien, asilar estos puntos definiendo un umbral o `threshold`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiHO1RGz6fy_"
      },
      "source": [
        "# ---\n",
        "# detectar anomalías dentro del X_test\n",
        "\n",
        "# definir umbral de rmse\n",
        "threshold = \n",
        "\n",
        "# obtener samples que superen el umbral\n",
        "anomalies = np.where( )\n",
        "\n",
        "# calcular accuracy de detección de series temporales degradadas\n",
        "accuracy = \n",
        "\n",
        "# print cantidad de anomalias detectadas\n",
        "print('detected anomalies: ', anomalies.size)\n",
        "print('accuracy: {:2.2f}%'.format(accuracy))\n",
        "\n",
        "# ---\n",
        "# visualizar en scatter plot\n",
        "plt.figure( figsize=(15, 5) )\n",
        "plt.title('Destected anomalies')\n",
        "plt.xlabel('Sample'); plt.ylabel('Reconstruction RMSE')\n",
        "\n",
        "plt.scatter(np.arange(rmse_test.size), rmse_test,\n",
        "            c=list(sample_label), cmap='viridis', vmax=2.0, alpha=0.5, s=40)\n",
        "\n",
        "plt.scatter(anomalies, rmse_test[anomalies], c='r', s=40)\n",
        "plt.plot([0, rmse_test.size], [threshold, threshold], c='r')\n",
        "plt.grid(True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}