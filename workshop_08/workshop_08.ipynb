{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_08.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N__0HSASS4Mt"
      },
      "source": [
        "# Generative Adversarial Networks\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_08/bin/gan_sample_2.png\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40KlGICGTBkF"
      },
      "source": [
        "## Raytracing Dataset\r\n",
        "\r\n",
        "Para generar los datos o bien, las imágenes reales que utilizaremos para entrenar nuestro modelo, utilizaremos el algoritmo de `raytracing` desarrollado por James Bowman. Este algoritmo disponible en el github `https://github.com/mdoege/raytrace` nos permite renderizar esferas parametrizables en un espacio tridimensional, como se muestra en la animación.\r\n",
        "\r\n",
        "![animation](https://github.com/mdoege/raytrace/raw/master/im.gif \"animation\")\r\n",
        "\r\n",
        "De este modo, utilizaremos este programa para generar una serie de imágenes (`samples`) de `128x128px` que contengan una esfera en distintas posiciones en el espacio. Para ahorrar tiempo y concentrarnos en el desarrollo del modelo, este proceso ya ha sido realizado y el dataset `GAN_dataset_128px.npz` correspondiente ha sido cargado a un Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_HTOSKTJleP"
      },
      "source": [
        "!pip install -U -q PyDrive\r\n",
        "\r\n",
        "import os\r\n",
        "from pydrive.auth import GoogleAuth\r\n",
        "from pydrive.drive import GoogleDrive\r\n",
        "from google.colab import auth\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "\r\n",
        "# inicializar GoogleDrive con credenciales de autorización\r\n",
        "auth.authenticate_user()\r\n",
        "gauth = GoogleAuth()\r\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\r\n",
        "drive = GoogleDrive(gauth)\r\n",
        "\r\n",
        "# crear carpeta para descargar los archivos .npz\r\n",
        "!mkdir /content/datasets\r\n",
        "\r\n",
        "# Google Drive IDs para descargar los archivos .npz\r\n",
        "files_id = [('GAN_dataset_128px.npz', '1kSOTgEj9oSOXEb_2LkTQfkPdh3fFi5Eq')]\r\n",
        "\r\n",
        "# comenzar descarga\r\n",
        "print('descargando datasets: ', end='')\r\n",
        "\r\n",
        "for filename, id in files_id:\r\n",
        "  save_path = os.path.join('/content/datasets', filename)\r\n",
        "\r\n",
        "  # descargar y guardar en /content/datasets\r\n",
        "  downloaded = drive.CreateFile({'id': id}) \r\n",
        "  downloaded.GetContentFile(save_path)\r\n",
        "\r\n",
        "# indicar descarga terminada\r\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUCAib3HU4iJ"
      },
      "source": [
        "Como ya es costumbre, carguemos este archivo mediante `np.load()` y exploremos las estructuras y datos que contiene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zfa1sRlJn6N"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# ---\r\n",
        "# cargar archivo GAN_dataset_128px.npz\r\n",
        "dataset = np.load('/content/datasets/GAN_dataset_128px.npz', allow_pickle=True)\r\n",
        "\r\n",
        "# print keys del dataset\r\n",
        "print('dataset.keys: ',  list( dataset.keys() ) )\r\n",
        "\r\n",
        "# ---\r\n",
        "# extraer conjuntos de imágenes y normalizar en [0., 1.]\r\n",
        "X = dataset['X']\r\n",
        "X = X/255.0\r\n",
        "\r\n",
        "# visualizar muestra del dataset\r\n",
        "sample_idx = np.random.choice( np.arange(X.shape[0]), 5 )\r\n",
        "img_sample = [ X[i, :, :, :].reshape( (128, 128, 3) ) for i in sample_idx ]\r\n",
        "img_sample = np.hstack(img_sample)\r\n",
        "\r\n",
        "plt.figure( figsize=(12, 12) )\r\n",
        "plt.imshow(img_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyR6XvXrVE5R"
      },
      "source": [
        "Así, como se mencionó anteriormente, el dataset cuenta con imágenes de `128x128px` con esferas de color variable que se posicionan aleatoriamente sobre el plano de ajedrez. De este modo, contamos con 2000 imágenes para el entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg19Q_6nWTQX"
      },
      "source": [
        "---\r\n",
        "## Model Setup\r\n",
        "\r\n",
        "En términos simples, un modelo GAN se compone de dos modelos independientes, el `Discriminador` y, por supuesto, el `Generador`.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_08/bin/GAN_diagram.png\" width=\"600\">\r\n",
        "\r\n",
        "Por un lado, el `Discriminador`, como el nombre lo indica, está diseñado para discriminar las imágenes creadas artificalmente por el `Generador` de las imágenes reales contenidas en el dataset. De este modo, este modelo consiste simplemente en un modelo Convolucional de Clasificación (CNN), tal como estudiamos en el `workshop_03`.\r\n",
        "\r\n",
        "Así, el `Discriminador` procesa secuencialmente la información contenida en la imagen de entrada mediante una serie de capas `Conv2D`, y posteriormente capas `Dense`, para finalmente retornar la etiqueta de clasificación de la imagen (`real` o `fake`) mediante una capa softmax.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_08/bin/DS_diagram.png\" height=\"400\">\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdtyoV3qJtAg"
      },
      "source": [
        "from keras.models import Model\r\n",
        "\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import MaxPooling2D\r\n",
        "\r\n",
        "# ---\r\n",
        "# primero debemos configurar nuestra capa Input donde debemos especificar\r\n",
        "# las dimensiones de los datos que se ingresarán al modelo\r\n",
        "# en este caso el discriminador recibe las imágenes de (300, 300, 3)\r\n",
        "input_dim = ( 128, 128, 3 )\r\n",
        "discriminator_input = Input( shape=input_dim )\r\n",
        "\r\n",
        "# ---\r\n",
        "# ahora, como cualquier clasificador de imágenes, debemos ir agregando\r\n",
        "# nuestras capas Conv2D y Pooling.\r\n",
        "\r\n",
        "# las keras.layers.Conv2D reciben la cantidad de filtros dentro de la capa,\r\n",
        "# el tamaño de estos filtros y la función de activación con que operarán.\r\n",
        "# https://keras.io/api/layers/convolution_layers/convolution2d/\r\n",
        "\r\n",
        "# las keras.layers.MaxPooling2D reciben el tamaño de la ventana sobre\r\n",
        "# la cual llevarán a cabo el down-sampling\r\n",
        "# https://keras.io/api/layers/pooling_layers/max_pooling2d/\r\n",
        "discriminator = Conv2D(32, (5, 5), activation='relu', padding='same')(discriminator_input)\r\n",
        "discriminator = Conv2D(32, (5, 5), activation='relu', padding='same')(discriminator)\r\n",
        "discriminator = MaxPooling2D( pool_size=(2, 2) )(discriminator)\r\n",
        "\r\n",
        "discriminator = Conv2D(64, (5, 5), activation='relu', padding='same')(discriminator)\r\n",
        "discriminator = Conv2D(64, (5, 5), activation='relu', padding='same')(discriminator)\r\n",
        "discriminator = MaxPooling2D( pool_size=(2, 2) )(discriminator)\r\n",
        "\r\n",
        "discriminator = Conv2D(48, (3, 3), activation='relu', padding='same')(discriminator)\r\n",
        "discriminator = Conv2D(48, (3, 3), activation='relu', padding='same')(discriminator)\r\n",
        "discriminator = MaxPooling2D( pool_size=(2, 2) )(discriminator)\r\n",
        "\r\n",
        "discriminator = Conv2D(32, (3, 3), activation='relu', padding='same')(discriminator)\r\n",
        "discriminator = Conv2D(32, (3, 3), activation='relu', padding='same')(discriminator)\r\n",
        "discriminator = MaxPooling2D( pool_size=(2, 2) )(discriminator)\r\n",
        "\r\n",
        "# ---\r\n",
        "# ahora debemos ir agregando nuestras capas Dense para procesar la\r\n",
        "# información hasta la capa de salida.\r\n",
        "# https://keras.io/api/layers/core_layers/dense/\r\n",
        "discriminator = Flatten()(discriminator)\r\n",
        "discriminator = Dropout( rate=0.2 )(discriminator)\r\n",
        "\r\n",
        "discriminator = Dense(units=256, activation='relu')(discriminator)\r\n",
        "discriminator = Dense(units=128, activation='relu')(discriminator)\r\n",
        "discriminator = Dropout( rate=0.2 )(discriminator)\r\n",
        "\r\n",
        "discriminator = Dense(units=128, activation='relu')(discriminator)\r\n",
        "discriminator = Dense(units=64, activation='relu')(discriminator)\r\n",
        "\r\n",
        "# ---\r\n",
        "# por último, como siempre, debemos configurar nuestra capa de salida\r\n",
        "# dado que el modelo discriminador consiste en simplemente un modelo de\r\n",
        "# clasificación emplearemos la función softmax, donde cada nodo indicará la\r\n",
        "# probabilidad de que los datos correspondan a una de las etiquetas.\r\n",
        "labels_num = 2\r\n",
        "discriminator_output = Dense(units=labels_num, activation='softmax')(discriminator)\r\n",
        "\r\n",
        "# ---\r\n",
        "# ahora configuraremos el modelo discriminador\r\n",
        "DS = Model(discriminator_input, discriminator_output)\r\n",
        "\r\n",
        "# print model.summary()\r\n",
        "DS.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvqllGkHax-K"
      },
      "source": [
        "Por el otro lado, el `Generador` está diseñado para construir imágenes a partir de un vector de ruido uniforme `NOISE_VECTOR`. Por supuesto, el tamaño de este vector o `NOISE_DIM` será otro de los hiperparámetros que constituirá el modelo.\r\n",
        "\r\n",
        "De este modo, de manera inversa a un modelo Convolucional convencional, el `Generador` procesa inicialmente este vector de ruido mediante una serie de capas `Dense` para luego dar paso a una reconfiguración del vector latente resultante a una forma compatible con las estructuras convolucionales `(n_sample, height, width, filters)` mediante una capa `Reshape`. Así, de forma simétrica a la estructura del `Discriminador`, una serie de capas `Conv2DTranspose` y `UpSampling2D` se encargan de ir construyendo secuencialmente una imagen RGB final de `128x128px` compatible con las características de las imágenes reales. Para compilar esta última imagen, la salida de este modelo se compone de una capa `Conv2D` de tres filtros (tres canales RGB).\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_08/bin/GN_diagram.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbf4IKzoJvXG"
      },
      "source": [
        "from keras.models import Model\r\n",
        "\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Reshape\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import Conv2DTranspose\r\n",
        "from keras.layers import UpSampling2D\r\n",
        "\r\n",
        "# ---\r\n",
        "# en el caso del modelo generador, este recibe como input el vector latente de\r\n",
        "# ruido a partir del cual construirá la imagen.\r\n",
        "# de este modo la dimensión de entrada de la capa Input depende del tamaño\r\n",
        "# de este vector noise.\r\n",
        "input_dim = ( 128, )\r\n",
        "generator_input = Input( shape=input_dim )\r\n",
        "\r\n",
        "# ---\r\n",
        "# ahora para transformar el vector noise en una imagen primero es necesario\r\n",
        "# agregar capas Dense con la finalidad de obtener un vector que puede ser\r\n",
        "# reconfigurado como imagen.\r\n",
        "# https://keras.io/api/layers/core_layers/dense/\r\n",
        "generator = Dense(units=128, activation='relu')(generator_input)\r\n",
        "generator = Dense(units=256, activation='relu')(generator)\r\n",
        "\r\n",
        "generator = Dense(units=8*8*32, activation='relu')(generator)\r\n",
        "generator = Reshape(target_shape=(8, 8, 32))(generator)\r\n",
        "\r\n",
        "# para generar o bien, construir la imagen de salida, utilizaremos capas\r\n",
        "# Conv2DTranspose y UpSampling2D hasta alcanzar las mismas características\r\n",
        "# que las imágenes del dataset.\r\n",
        "# https://keras.io/api/layers/convolution_layers/convolution2d_transpose/\r\n",
        "# https://keras.io/api/layers/reshaping_layers/up_sampling2d/\r\n",
        "generator = UpSampling2D( size=(2, 2) )(generator)\r\n",
        "generator = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(generator)\r\n",
        "generator = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(generator)\r\n",
        "\r\n",
        "generator = UpSampling2D( size=(2, 2) )(generator)\r\n",
        "generator = Conv2DTranspose(48, (3, 3), activation='relu', padding='same')(generator)\r\n",
        "generator = Conv2DTranspose(48, (3, 3), activation='relu', padding='same')(generator)\r\n",
        "\r\n",
        "generator = UpSampling2D( size=(2, 2) )(generator)\r\n",
        "generator = Conv2DTranspose(64, (5, 5), activation='relu', padding='same')(generator)\r\n",
        "generator = Conv2DTranspose(64, (5, 5), activation='relu', padding='same')(generator)\r\n",
        "\r\n",
        "generator = UpSampling2D( size=(2, 2) )(generator)\r\n",
        "generator = Conv2DTranspose(64, (5, 5), activation='relu', padding='same')(generator)\r\n",
        "generator = Conv2DTranspose(64, (5, 5), activation='relu', padding='same')(generator)\r\n",
        "\r\n",
        "# ---\r\n",
        "# finalmente, utilizaremos una capa Conv2D de tres filtros para generar\r\n",
        "# la imagen de salida.\r\n",
        "generator_output = Conv2D(3, (5, 5), activation='linear', padding='same')(generator)\r\n",
        "\r\n",
        "# ---\r\n",
        "# ahora configuraremos el modelo discriminador\r\n",
        "GN = Model(generator_input, generator_output)\r\n",
        "\r\n",
        "# print model.summary()\r\n",
        "GN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2BEcSZedXtV"
      },
      "source": [
        "Ahora al compilar estos modelos, hay que tener presente que el `Discriminador` debe ser entrenado aisladamente, sin conexión con el `Generador`, tal como se muestra en su diagrama.\r\n",
        "\r\n",
        "Por otro lado, dado que el objetivo del `Generador` es confundir al `Discriminador`, su función de pérdida (`loss function`) está directamente ligada al output del `Discriminador`. Así, para su entrenamiento, el `Generador` debe ser compilado en conjunto con el `Discriminador`, pero con los pesos de éste último congelados. De este modo, la entrada de este modelo compuesto (i.e la GAN) es el vector de ruido (`NOISE_VECTOR`) del `Generador`, mientras que la salida corresponde a la probabilidad de clasificación computada por el `Discriminador`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y09OCyNJxUJ"
      },
      "source": [
        "from keras.optimizers import Adam\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "# ---\r\n",
        "# parámetros de la GAN\r\n",
        "NOISE_DIM = 128\r\n",
        "IMG_DIM = (128, 128, 3)\r\n",
        "\r\n",
        "# ---\r\n",
        "# compilar modelo discriminador\r\n",
        "opt = Adam( learning_rate=1e-4 )\r\n",
        "DS.compile(loss='binary_crossentropy', optimizer=opt, metrics=['mae'])\r\n",
        "    \r\n",
        "# ---\r\n",
        "# construir modelo GAN\r\n",
        "GAN = Sequential()\r\n",
        "GAN.add(GN)\r\n",
        "GAN.add(DS)\r\n",
        "\r\n",
        "# congelar el entrenamiento del discriminador\r\n",
        "DS.trainable = False\r\n",
        "\r\n",
        "# compilar modelo GAN\r\n",
        "GAN.compile(loss='binary_crossentropy', optimizer=opt, metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C66UZPy4gLZr"
      },
      "source": [
        "## Model Training\r\n",
        "\r\n",
        "Dado que internamente los modelos que componen a la GAN presentan objetivos distintos, el entrenamiento de esta arquitectura requiere de un entrenamiento secuencial iterativo, que permita el entrenamiento aislado tanto del `Discriminador` como del `Generador`.\r\n",
        "\r\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_08/bin/GAN_diagram.png\" width=\"600\">\r\n",
        "\r\n",
        "En primer lugar, al inicio de una iteración de entrenamiento, el `Generador` es utilizado para generar un cantidad `batch_size` de imágenes falsas. Posteriormente, este `batch` de imágenes falsas es combinado con un `batch` de imágenes reales extraídas del dataset original, para luego ser alimentado al entrenamiento del `Discriminador`. Como cualquier otro clasificador, el `Discriminador` es entrenado siguiendo una `binary_crossentropy` para aprender a discernir entre los dos sets de datos.\r\n",
        "\r\n",
        "Una vez que el `Discriminador` ha sido entrenado sobre este `batch` de imágenes, este es congelado para el entrenamiento del `Generador`. A parir de un `batch` de vectores de ruido uniforme el `Generador` es entrenado para generar imágenes que confundan al `Discriminador` de la GAN. Es decir, la función de pérdida del `Generador` apunta a que el `Discriminador` clasifique como `reales` las imágenes generadas artificialmente.\r\n",
        "\r\n",
        "Terminado el entrenamiento del `Generador`, los pesos del `Discriminador` son restaurados como entrenables y el proceso se repite hasta finalizar con las épocas de entrenamiento especificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNWr6VKLJzsA"
      },
      "source": [
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from IPython.display import clear_output\r\n",
        "from progressbar import ProgressBar\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "# ---\r\n",
        "# configurar parámetros de entrenamiento\r\n",
        "\r\n",
        "# número de updates para cada iteración de entrenamiento\r\n",
        "GLOBAL_UPDATES = 100\r\n",
        "DS_UPDATES = 20\r\n",
        "GN_UPDATES = 20\r\n",
        "\r\n",
        "# batch_size para cada update de entrenamiento\r\n",
        "BATCH_SIZE = 64\r\n",
        "NUM_EPOCHS = 200\r\n",
        "\r\n",
        "# parámetro de monitoreo\r\n",
        "MONITOR_IT = 1\r\n",
        "\r\n",
        "# ---\r\n",
        "# función de extracción de imágenes reales para generar los batches\r\n",
        "def get_real_images(X, batch_size):\r\n",
        "  \"\"\"\r\n",
        "  -> np.array()\r\n",
        "\r\n",
        "  extrae de manera aleatoria 'batch_size' imágenes del dataset X.\r\n",
        "\r\n",
        "  :param np.array X:\r\n",
        "    dataset que contiene las imágenes a extraer.\r\n",
        "  :param int batch_size:\r\n",
        "    cantidad de imágenes a extraer.\r\n",
        "  \r\n",
        "  :returns: np.array de la forma (batch_size, height, width, channels) \r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  # obtener index a extraer aleatoriamente\r\n",
        "  samples_idx = np.random.choice( np.arange(X.shape[0]), batch_size)\r\n",
        "\r\n",
        "  # obtener las imágenes para el batch\r\n",
        "  X_real = X[samples_idx, :, :, :]\r\n",
        "  return X_real\r\n",
        "\r\n",
        "# ---\r\n",
        "# inicializar listas de función de pérdida\r\n",
        "avg_loss_DS = []\r\n",
        "avg_loss_GN = []\r\n",
        "total_it = 0\r\n",
        "\r\n",
        "# training loop\r\n",
        "for epoch in range(NUM_EPOCHS):\r\n",
        "\r\n",
        "  # inicializar listas internas\r\n",
        "  loss_DS = []\r\n",
        "  loss_GN = []\r\n",
        "\r\n",
        "  for it in range(GLOBAL_UPDATES):\r\n",
        "    # ---\r\n",
        "    # discriminator training loop\r\n",
        "    DS.trainable = True\r\n",
        "    bar = ProgressBar()\r\n",
        "    for i in bar( range(DS_UPDATES) ):\r\n",
        "      # obtener batch de imágenes reales (1)\r\n",
        "      X_real = get_real_images(X, BATCH_SIZE)\r\n",
        "      Y_real = np.ones( (BATCH_SIZE, 1) )\r\n",
        "\r\n",
        "      # generar batch de imágenes falsas (0)\r\n",
        "      noise_vector = np.random.randn(BATCH_SIZE, NOISE_DIM)\r\n",
        "      X_fake = GN.predict(noise_vector)\r\n",
        "      Y_fake = np.zeros( (BATCH_SIZE, 1) )\r\n",
        "\r\n",
        "      # compilar batch para entrenamiento\r\n",
        "      X_DS = np.vstack([X_real, X_fake])\r\n",
        "      Y_DS = np.vstack([Y_real, Y_fake])\r\n",
        "      Y_DS = to_categorical(Y_DS, num_classes=2)\r\n",
        "\r\n",
        "      batch_shuffle = np.random.permutation( np.arange(2*BATCH_SIZE) )\r\n",
        "      X_DS = X_DS[batch_shuffle, :, :, :]\r\n",
        "      Y_DS = Y_DS[batch_shuffle, :]\r\n",
        "\r\n",
        "      # entrenar sobre el batch de imágenes, mediante train_on_batch\r\n",
        "      DS_loss = DS.train_on_batch( X_DS, Y_DS )[1]\r\n",
        "\r\n",
        "    # ---\r\n",
        "    # visualizar imágenes de muestra para monitorear el entrenamiento\r\n",
        "    if (total_it % MONITOR_IT == 0):\r\n",
        "      # obtener 5 imágenes reales\r\n",
        "      imgs_real = get_real_images(X, 5)\r\n",
        "\r\n",
        "      # generar 9 imágenes falsas\r\n",
        "      noise_vec = np.random.randn(9, NOISE_DIM)\r\n",
        "      imgs_fake = GN.predict(noise_vec)\r\n",
        "\r\n",
        "      # para cada set de imágenes\r\n",
        "      for img_set in [imgs_fake, imgs_real]:\r\n",
        "        plt.figure( figsize=(15, 3) )\r\n",
        "\r\n",
        "        for i in range(5):\r\n",
        "          # obtener imagen\r\n",
        "          img = img_set[i, :, :, :].reshape( (128, 128, 3) )\r\n",
        "          img = np.clip(img, 0.0, 1.0)\r\n",
        "\r\n",
        "          # obtener clasificación del discriminador\r\n",
        "          x = img.reshape( (1, 128, 128, 3) )\r\n",
        "          DS_pred = DS.predict(x)[0, 1]\r\n",
        "\r\n",
        "          # visualizar\r\n",
        "          plt.subplot(1, 5, i+1)\r\n",
        "          plt.title( 'score: {:1.2f}'.format(DS_pred) )\r\n",
        "          plt.imshow(img)\r\n",
        "      clear_output(True)\r\n",
        "      plt.show()\r\n",
        "\r\n",
        "      # ---\r\n",
        "      # generator training loop\r\n",
        "      DS.trainable = False\r\n",
        "      GN_loss = 0\r\n",
        "\r\n",
        "      bar = ProgressBar()\r\n",
        "\r\n",
        "      for i in bar( range(GN_UPDATES) ):\r\n",
        "        # generar batch de imágenes falsas\r\n",
        "        X_GN = np.random.randn(BATCH_SIZE, NOISE_DIM)\r\n",
        "        Y_GN = np.ones( (BATCH_SIZE, 1) )\r\n",
        "        Y_GN = to_categorical(Y_GN, num_classes=2)\r\n",
        "\r\n",
        "        # entrenar sobre el batch de imágenes, mediante train_on_batch\r\n",
        "        GN_loss += GAN.train_on_batch(X_GN, Y_GN)[1]\r\n",
        "\r\n",
        "    # ---\r\n",
        "    # registrar training_history\r\n",
        "    loss_DS.append( DS_loss )        \r\n",
        "    loss_GN.append( GN_loss/GN_UPDATES )\r\n",
        "    total_it += 1\r\n",
        "\r\n",
        "  # ---\r\n",
        "  # visualizar función de pérdida\r\n",
        "  avg_loss_DS.append( np.mean(loss_DS) )\r\n",
        "  avg_loss_GN.append( np.mean(loss_GN) )\r\n",
        "\r\n",
        "  plt.figure( figsize=(15, 5) )\r\n",
        "  plt.plot( range(len(avg_loss_DS)), avg_loss_DS )\r\n",
        "  plt.plot( range(len(avg_loss_GN)), avg_loss_GN )\r\n",
        "  plt.legend(['discriminator loss', 'generator loss'])\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}