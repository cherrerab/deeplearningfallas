{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_07.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5-uQAlYGlk"
      },
      "source": [
        "# C-MAPSS Anomaly Detection Variational Autoencoder\n",
        "\n",
        "Como ya vimos en el workshop anterior, la detección de anomalías (Anomaly Detection) sobre sistemas con una alta dimensionalidad de datos, es un problema de particular interés en la operación y mantenimiento de activos. En particular, implementamos un Autoencoder sobre las series temporales nominales de CMAPSS con la finalidad de reconocer patrones o colecciones de datos que escapan del estado nominal del sistema analizado, i.e `anomalies`.\n",
        "\n",
        "Ahora, si bien los Autoencoders son capaces de reducir la dimensionalidad de una determinada estructura de datos a un vector latente mediante su etapa de `encoding`, estos tienen el problema de que finalmente procesan o codifican cada sample de manera independiente. En otras palabras, si dos vectores latentes generados por el `encoder` son similares, esto no garantiza que sus reconstrucciones sean similares, por lo que la representación del vector latente pasa a ser parte de la caja negra del autoencoder. En este sentido, considerando que todos los datos o samples utilizados para el `training` del modelo son de la misma clase o describen el mismo sistema, nos gustaría que el `encoding` del vector latente sea capaz de capturar la distribución de los datos en su nueva representación.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1000/0*uq2_ZipB9TqI9G_k\" width=\"600\">\n",
        "\n",
        "Aquí es donde entran los Variational Autoencoders (VAE) en los cuales, en términos generales, se impone (forzadamente) una distribución normal sobre cada uno de los componentes del vector latente. De este modo, como la distribución normal $\\mathcal{N}(0, \\sigma^2)$ se caracteriza por el promedio y la varianza de la distribución, la arquitectura VAE contiene estructuras que calculan y regulan estos parámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbZSPLJ9xoS-"
      },
      "source": [
        "## C-MAPSS Dataset\n",
        "\n",
        "Como recordatorio, el C-MAPSS dataset que utilizaremos en esta ocación consiste en múltiples simulaciones de degradación de motores turbofán bajo distintas condiciones, tanto mecánicas como operacionales. En este sentido, el dataset se compone de más de `27000` series temporales de los sensores del turbofán que, gracias a la simulación de este, han podido ser asociadas al RUL del motor.\n",
        "\n",
        "En esta sección nos encargaremos de cargar y explorar el dataset que utilizaremos, denominado `C-MAPSS FD001`, que contiene `100` simulaciones de motores mecánicamente distintos bajo condiciones nominales de operación. Nuevamente, para facilitar la carga del dataset a este entorno de Google Colab el achivo `CMAPSS_FD001.npz` ha sido cargado a un Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ewoTBvxrf4"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# inicializar GoogleDrive con credenciales de autorización\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# crear carpeta para descargar los archivos .npz\n",
        "!mkdir /content/datasets\n",
        "\n",
        "# Google Drive IDs para descargar los archivos .npz\n",
        "files_id = [('CMAPSS_FD001.npz', '1DNJFrQKB4I7SqjpFmM5SsKRaN7O9XCNZ')]\n",
        "\n",
        "# comenzar descarga\n",
        "print('descargando datasets: ', end='')\n",
        "\n",
        "for filename, id in files_id:\n",
        "  save_path = os.path.join('/content/datasets', filename)\n",
        "\n",
        "  # descargar y guardar en /content/datasets\n",
        "  downloaded = drive.CreateFile({'id': id}) \n",
        "  downloaded.GetContentFile(save_path)\n",
        "\n",
        "# indicar descarga terminada\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbT8hv1zxtLH"
      },
      "source": [
        "Carguemos este archivo mediante `np.load()` y exploremos las estructuras y datos que contiene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow5dLpTnxutI"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# cargar archivo CMAPSS_FD001.npz\n",
        "dataset = np.load('/content/datasets/CMAPSS_FD001.npz', allow_pickle=True)\n",
        "\n",
        "# print keys del dataset\n",
        "print(  list( dataset.keys() ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibfv3i0mxxQo"
      },
      "source": [
        "También, como ya es rutinario, aprovecharemos de cargar el GitHub del curso deeplearningfallas para disponer de algunas utilidades que nos serán de utilidad más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egctnLdBxyuB"
      },
      "source": [
        "# cargar GitHub https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/\n",
        "!git clone https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/deeplearningfallas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ypCbvHgx28v"
      },
      "source": [
        "---\n",
        "# Training Data\n",
        "\n",
        "Como ya sabemos, el dataset se encuentra previamente separado en conjuntos de `training` y `testing`. En particular, cada uno de estos conjuntos están separados en `100` simulaciones de turbinas distintas. Luego, cada una de estas simulaciones contiene varias muestras de series temporales registradas durante la operación que están asociadas a un valor RUL en el conjunto `Y_train` o `Y_test`.\n",
        "\n",
        "Nuevamente, para generar los conjuntos `training` y `testing` que finalmente utilizaremos para desarrollar nuestros modelos, debemos concatenar las series de todas las simulaciones de `C-MAPSS` y modificar sus dimensiones para que sean compatibles con las capas y estructuras de `tensorflow` y `keras`. En particular, recordemos que los datos de cada simulación en el conjunto `X_train` son de la forma `(n_samples, n_timesteps, n_features, 1)`. Donde en este caso `n_features` corresponde a la cantidad de sensores dentro de la simulación (`14`), mientras `n_timesteps` corresponde a la cantidad de puntos temporales o mediciones dentro de la serie (`30`). Por otro lado, los datos del conjunto `Y_train` son de la forma `(n_samples, 1)`, donde cada valor corresponde al RUL de las respectivas series temporales en el conjunto `X_train`.\n",
        "\n",
        "En este caso, a diferencia del workshop anterior, aplanaremos las series temporales de cada sample en un único vector de la forma `(n_timesteps*n_features, )`, pues consideraremos cada punto dentro del registro temporal como una dimensión que define el espacio vectorial de nuestra colección de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7mVcgZUx2G_"
      },
      "source": [
        "# extraer conjuntos de training y testing del dataset\n",
        "X_train, Y_train = dataset['x_train'], dataset['y_train']\n",
        "X_test, Y_test = dataset['x_test'], dataset['y_test']\n",
        "\n",
        "# ---\n",
        "# concatenar y reconfigurar datos de las 100 simulaciones CMAPSS\n",
        "# en este caso reordenaremos cada sample de la forma (n_timesteps*n_features)\n",
        "# para ingresarlos al autoencoder que desarrollaremos.\n",
        "\n",
        "# los datos X deben ser de la forma (n_samples, n_timesteps*n_features)\n",
        "X_train = np.vstack( X_train )\n",
        "X_train = np.reshape( X_train, (-1, 30*14) )\n",
        "\n",
        "X_test = np.vstack( X_test )\n",
        "X_test = np.reshape( X_test, (-1, 30*14) )\n",
        "\n",
        "# los datos Y deben ser de la forma (n_samples, 1)\n",
        "Y_train = np.vstack( Y_train )\n",
        "Y_train = np.reshape( Y_train, (-1, 1) )\n",
        "\n",
        "Y_test = np.vstack( Y_test )\n",
        "Y_test = np.reshape( Y_test, (-1, 1) )\n",
        "\n",
        "# print dimensiones de los nuevos conjuntos\n",
        "print('X_train.shape: ', X_train.shape)\n",
        "print('Y_train.shape: ', Y_train.shape)\n",
        "print('\\nX_test.shape: ', X_test.shape)\n",
        "print('Y_test.shape: ', Y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acc_ILRLx62b"
      },
      "source": [
        "Ahora, como se mencionó en la introducción de este workshop, para el entrenamiento del `autoencoder` que desarrollaremos utilizaremos únicamente datos o `samples` que presenten una condición `nominal` de operación. Arbitrariamente definirimos esta condición en función del RUL de cada sample, donde aquellos que presenten un RUL menor a 0.3, se encontrarán en un estado de degradación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzMj2AY4x8Sq"
      },
      "source": [
        "# ---\n",
        "# segementar samples que presenten una serie temporal de operación 'nominal'\n",
        "# en este caso, un sample 'nominal' es aquel cuyo RUL > 0.3\n",
        "\n",
        "# obtener indices nominal_idx de samples nominales\n",
        "# mediante np.where\n",
        "RUL_train = Y_train.flatten() \n",
        "nominal_idx = np.where( RUL_train > 0.3 )[0]\n",
        "\n",
        "# segementar X_train con samples nominales\n",
        "X_train_nominal = X_train[nominal_idx, :]\n",
        "\n",
        "# print dimensiones de este nuevo conjunto\n",
        "print('X_train_nominal.shape: ', X_train_nominal.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBfixTBCx_Zg"
      },
      "source": [
        "Como tenemos una gran cantidad de datos tanto en el conjunto de `training`, como en el de `testing`, siguiendo el procedimiento apropiado para el desarrollo de modelos de deep learning, utilizaremos la función `sklearn.train_test_split` para generar un conjunto de `validation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzImPcxpx-vQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---\n",
        "# realizar data splitting para generar los conjuntos (X_train_AE, X_val_AE)\n",
        "# para el entrenamiento del autoencoder.\n",
        "\n",
        "# dado que los autoencoders son modelos de reeconstrucción, en este caso\n",
        "# no es necesario extraer conjuntos Y.\n",
        "X_train_AE, X_val_AE, _, _ = train_test_split(X_train_nominal, X_train_nominal,\n",
        "                                              train_size=0.8 , test_size=0.2 ,\n",
        "                                              random_state=217 )\n",
        "\n",
        "# print sample distribution\n",
        "print( 'train split: {:d} samples'.format(X_train_AE.shape[0]) )\n",
        "print( '\\nvalidation split: {:d} samples'.format(X_val_AE.shape[0]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v4utNpefuAz"
      },
      "source": [
        "---\n",
        "# Model Building\n",
        "\n",
        "Ahora comenzaremos a implementar el `variational autoencoder` en que se centra el trabajo de este workshop. Finalmente, un `variational autoencoder` no es un muy diferente del `vanilla autoencoder` en cuanto a la estructura general de `encoder` y `decoder`. No obstante, las VAE incorporan estructuras que se encargan de realizar un `random sampling` sobre la distribución normal para generar el vector latente.\n",
        "\n",
        "Para esto, los `variational autoencoder` incorporan dos capas `Dense` paralelas que representan el `mu` y la `log_variance` del sample procesado. Luego, estas dos representaciones son utilizadas para realizar el `random sampling` y generar el `latent_vector` mediante una función predefinida `normal_random_sampling`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8ojU4-mjYRk"
      },
      "source": [
        "import keras.backend as kb\n",
        "\n",
        "\n",
        "def normal_random_sampling(mu_log_variance):\n",
        "  \"\"\"\n",
        "  -> np.array\n",
        "\n",
        "  extrae el vector latente de la distribución normal (multidimensional) descrita\n",
        "  por los parámetros mu, log_variance = mu_log_variance\n",
        "\n",
        "  :param tuple(np.array) mu_log_variance:\n",
        "    tupla que contiene los vectores mu y log_variance del sample.\n",
        "\n",
        "  :returns:\n",
        "    random sample sobre la distribución normal definida por estos parámetros.\n",
        "  \"\"\"\n",
        "\n",
        "  # extraer parámetros\n",
        "  mu, log_variance = \n",
        "\n",
        "  # obtener dimension del vector latente mediante keras.backend.shape\n",
        "  latent_shape = \n",
        "\n",
        "  # obtener random sample sobre la distribución normal estándar\n",
        "  # mediante keras.backend.random_normal(shape, mean, stddev)\n",
        "  epsilon = \n",
        "\n",
        "  # ajustar random sample a la distribución N(mu, var)\n",
        "  random_sample = mu + kb.exp(log_variance/2) * epsilon\n",
        "  \n",
        "  return random_sample"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygmalp4lmYXd"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "Para configurar nuestro modelo VAE, del mismo modo que como lo hicimos con el `vanilla autoencoder`, debemos definir separadamente los modelos `encoder` y `decoder` que lo compondrán. Similar a la figura de arriba, para configurar nuestro modelo de regresión utilizaremos nuevamente la librería `keras` o `tf.keras` y se compondrá principalmente de capas `Dense`, también llamadas Fully Connected en la literatura.\n",
        "\n",
        "- https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "De esta forma, en términos generales, compondremos nuestro `encoder` de una serie de capas `Dense`, que se encargarán de procesar la información y los features de los datos de entrada hasta dos capas `Dense` `encoder_mu` y `encoder_log_var` que serán luego ingresadas a la función `normal_random_sampling`, mediante una capa `Lambda`, para generar el `latent_vector`. De la misma manera, ya sea simétricamente o no, el `decoder` recibirá como entrada el `latent_vector` y mediante otra serie de capas `Dense` reconstruirá los datos originales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAUf02KPmXnN"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Lambda\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "# en este caso crearemos el autoencoder usando la API Functional\n",
        "# la API Functional de keras permite la creación de modelos de una\n",
        "# manera mucho más flexible\n",
        "# https://keras.io/guides/functional_api/\n",
        "\n",
        "# ---\n",
        "# primero debemos crear nuestra capa Input donde debemos especificar\n",
        "# las dimensiones de los datos que se ingresarán al modelo\n",
        "# en este caso el autoencoder recibe samples de la forma (420, )\n",
        "input_dim = ( 420, )\n",
        "input_layer = Input( shape=input_dim )\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas Dense para configurar\n",
        "# el modelo encoder.\n",
        "# https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "encoder = Dense(units=256, activation='relu')(input_layer)\n",
        "encoder = Dense(units= , activation= )(encoder)\n",
        "\n",
        "# ahora configuraremos el par de capas Dense que representarán\n",
        "# el mu y el log_variance del sample.\n",
        "# estas deben poseer la misma dimensión que el vector latente.\n",
        "latent_dim = \n",
        "encoder_mu = \n",
        "encoder_log_var = \n",
        "\n",
        "# utilizando una capa keras.Lambda podemos ingresar estos dos vectores\n",
        "# a la función normal_random_sampling y considerar este cálculo como\n",
        "# otra etapa en la arquitectura.\n",
        "# las keras.Lambda están diseñadas justamente para incorporar funciones\n",
        "# arbitrarias o personalizadas dentro de los modelos\n",
        "# https://keras.io/api/layers/core_layers/lambda/\n",
        "latent_vector = \n",
        "\n",
        "# finalmente, debemos ir agregando nuestras capas Dense para configurar\n",
        "# el decoder de nuestro modelo.\n",
        "# en este caso hay que tener presente que la salida de esta sección\n",
        "# debe tener la misma dimensionalidad o forma que los samples de entrada.\n",
        "\n",
        "decoder = Dense(units=128, activation='relu')(latent_vector)\n",
        "decoder = Dense(units=, activation=)(decoder)\n",
        "\n",
        "\n",
        "output_layer = Dense(units=, activation='linear')(decoder)\n",
        "\n",
        "# ---\n",
        "# ahora configuraremos el modelo autoencoder que entrenaremos\n",
        "VAE_model = \n",
        "\n",
        "# adicionalmente podemos configurar el modelo encoder\n",
        "# para utilizarlo como un reductor dimensional\n",
        "# esto resulta más práctico cuando la dimensión es 2 o 3\n",
        "encoder_model = \n",
        "\n",
        "# print model.summary()\n",
        "VAE_model.summary()\n",
        "\n",
        "# generar diagrama del modelo\n",
        "_ = plot_model(VAE_model, to_file='/content/model.png', show_shapes=True, show_layer_names=False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KzLY1zkr1JI"
      },
      "source": [
        "## Compile Model\n",
        "\n",
        "Como siempre, antes de poner a entrenar al modelo, es necesario realizar unas configuraciones adicionales. En particular, debemos especificar la función de pérdida o `loss function` que se optimizará durante el entrenamiento y el método de optimización como SGD o Adam.\n",
        "- https://keras.io/api/models/model_training_apis/\n",
        "- https://keras.io/api/optimizers/\n",
        "\n",
        "\n",
        "En este caso, a diferencia del `vanilla autoencoder` visto en el workshop anterior, los `variational autoencoders` no solo deben ser entrenados para reducir el error de reconstrucción, sino que como la idea es también imponer que cada uno de los componentes del vector latente siga una distribución normal estándar, este factor también debe ser incorporado en la función de pérdida. De este modo, debemos definir una función de pérdida personalizada para este caso particular."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRd9RfJztQ2G"
      },
      "source": [
        "import keras.backend as kb\n",
        "\n",
        "def loss_VAE(encoder_mu, encoder_log_var):\n",
        "  \"\"\"\n",
        "  -> float\n",
        "\n",
        "  función de pérdida para el entrenamiento de los variational autoencoders,\n",
        "  esta se compone de un factor de reconstrucción (vae_reconstruction_loss)\n",
        "  y otro correspondiente a la divergencia KL entre la distribución dada por\n",
        "  las capas encoder_mu y encoder_log_var, y la distribución normal estándar.\n",
        "  \"\"\"\n",
        "\n",
        "  def vae_reconstruction_loss(y_true, y_predict):\n",
        "    \"\"\"\n",
        "    error mse de reconstrucción.\n",
        "    \"\"\"\n",
        "    # ponderación del error de reconstrucción\n",
        "    reconstruction_loss_factor = 1000\n",
        "\n",
        "    # error cuadrado medio de reconstrucción\n",
        "    reconstruction_loss = kb.mean(kb.square(y_true-y_predict), axis=1)\n",
        "\n",
        "    return reconstruction_loss_factor * reconstruction_loss\n",
        "\n",
        "  def vae_kl_loss(encoder_mu, encoder_log_variance):\n",
        "    \"\"\"\n",
        "    error de divergencia Kullback-Leibler.\n",
        "    la divergencia Kullback–Leibler cuantifica que tan diferente es una\n",
        "    distribución probabilística respecto a otra.\n",
        "    \"\"\"\n",
        "    kl_loss = -0.5 * kb.sum(1.0 + encoder_log_variance - kb.square(encoder_mu) - kb.exp(encoder_log_variance), axis=1)\n",
        "    return kl_loss\n",
        "\n",
        "  def vae_kl_loss_metric(y_true, y_predict):\n",
        "    \"\"\"\n",
        "    error de divergencia Kullback-Leibler.\n",
        "    la divergencia Kullback–Leibler cuantifica que tan diferente es una\n",
        "    distribución probabilística respecto a otra.\n",
        "    \"\"\"\n",
        "    kl_loss = -0.5 * kb.sum(1.0 + encoder_log_variance - kb.square(encoder_mu) - kb.exp(encoder_log_variance), axis=1)\n",
        "    return kl_loss\n",
        "\n",
        "  def vae_loss(y_true, y_predict):\n",
        "    \"\"\"\n",
        "    ponderación final del error de reconstrucción y la divergencia KL.\n",
        "    \"\"\"\n",
        "    # error de reconstrucción (vae_reconstruction_loss)\n",
        "    reconstruction_loss = \n",
        "\n",
        "    # divergencia kullback-leibler (vae_kl_loss).\n",
        "    kl_loss = \n",
        "\n",
        "    # retornar suma de losses\n",
        "    loss = reconstruction_loss + kl_loss\n",
        "    return loss\n",
        "\n",
        "  return vae_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVRWI01hxG9A"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# configurar optimizador Adam\n",
        "# https://keras.io/api/optimizers/adam/\n",
        "opt = Adam( learning_rate=1e-3 )\n",
        "\n",
        "# ---\n",
        "# compilar modelo siguiendo la función de pérdida\n",
        "# que definimos en el bloque de código anterior\n",
        "VAE_model.compile( loss=  , optimizer=opt )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU0El1gFxeDR"
      },
      "source": [
        "## Model Training\n",
        "Hemos llegado a la parte final del proceso, para entrenar nuestro modelo debemos especificar los sets que utilizaremos para el proceso `(X_train, Y_train)`, la cantidad de `epochs` que durará el entrenamiento, y el `batch size` de muestras que se irán entregando al modelo a medida que este va iterativamente ajustando sus parámetros.\n",
        "\n",
        "En este caso, es necesario especificar que los datos `Y_train` del entrenamiento son equivalentes a los datos de entrada `X_train`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCinoNpLxkYJ"
      },
      "source": [
        "from utils import plot_loss_function\n",
        "\n",
        "# realizar rutina de entrenamiento\n",
        "train_history = VAE_model.fit(X_train_AE, X_train_AE,\n",
        "                              batch_size=256, epochs=150,\n",
        "                              validation_data=(X_val_AE, X_val_AE))\n",
        "\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(train_history, figsize=(10,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltcp_S5pyYPv"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Ahora, recordemos que la razón incial del desarrollo de este autoencoder es analizar si este puede ser utilizado para segementar o bien, permitir el reconocimiento de series temporales que presenten un estado de degradación crítico. De este modo, evaluaremos si existe una diferencia significativa entre el error de reconstrucción sobre los datos `nominales` y los `degradados`. Para emplear el `variational autoencoder` sobre nuevas series temporales, conviene utilizar el método `keras.Model.predict`, análogamente a como hemos hecho en las arquitecturas anteriores.\n",
        "\n",
        "Por otro lado, dado que estamos frente a un problema de regresión no podemos utilizar las herramientas de visualización anteriores como la matriz de consfusión. En este caso utilizaremos un `scatter plot` medainte la función `utils.plot_predict_scatter` para visualizar la correlación entre las predicciones `Y_pred` y los valores reales `Y_true`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOJI4OWtynHK"
      },
      "source": [
        "from utils import plot_predict_scatter\n",
        "\n",
        "# ---\n",
        "# obtener predicciones de X_test del modelo\n",
        "# en primer lugar, similar a como hicimos para X_train\n",
        "# debemos segmentar las series nominales de las degradadas.\n",
        "\n",
        "# obtener indices nominal_idx de samples nominales\n",
        "# mediante np.where\n",
        "RUL_test = Y_test.flatten() \n",
        "nominal_idx = np.where(  )[0]\n",
        "degraded_idx = np.where(  )[0]\n",
        "\n",
        "# segementar samples nominales y degradados\n",
        "X_test_nominal = X_test[nominal_idx, :]\n",
        "X_test_degraded = X_test[degraded_idx, :]\n",
        "\n",
        "# obtener recontrucciones del autoencoder\n",
        "AE_test_nominal = \n",
        "AE_test_degraded = \n",
        "\n",
        "# obtener el rmse de las reconstrucciones\n",
        "rmse_nominal = np.sqrt( np.mean( np.power(X_test_nominal - AE_test_nominal, 2 ), axis=1) )\n",
        "rmse_degraded = np.sqrt( np.mean( np.power(X_test_degraded - AE_test_degraded, 2 ), axis=1) )\n",
        "\n",
        "print('nominal rmse: {:1.3f}'.format(np.mean(rmse_nominal)))\n",
        "print('degraded rmse: {:1.3f}'.format(np.mean(rmse_degraded)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHltN_TVywzf"
      },
      "source": [
        "De esta manera, podemos ver que dentro del conjunto de series temporales degradadas existen puntos que escapan significativamente del rango de rmse. Podemos detectar o bien, asilar estos puntos definiendo un umbral o `threshold`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9bk5KEIyuIA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---\n",
        "# obtener predicciones sobre todo el test set\n",
        "AE_test = VAE_model(X_test)\n",
        "rmse_test = np.sqrt( np.mean( np.power(X_test - AE_test, 2 ), axis=1) )\n",
        "\n",
        "# sort test samples para segmentar grupos nominales y degradados\n",
        "sample_label = np.array( RUL_test <= 0.3, dtype=int )\n",
        "sample_sort = np.argsort(sample_label)\n",
        "\n",
        "sample_label = sample_label[sample_sort]\n",
        "rmse_test = rmse_test[sample_sort]\n",
        "\n",
        "# umbral de rmse\n",
        "threshold = 0.19\n",
        "\n",
        "# obtener sample que superen el umbral\n",
        "anomalies = np.where(rmse_test > threshold)[0]\n",
        "accuracy = 100*anomalies.size/degraded_idx.size\n",
        "\n",
        "# print cantidad de anomalias detectadas\n",
        "print('detected anomalies: ', anomalies.size)\n",
        "print('accuracy: {:2.2f}%'.format(accuracy))\n",
        "\n",
        "# ---\n",
        "# visualizar en scatter plot\n",
        "plt.figure( figsize=(12, 4) )\n",
        "plt.title('Destected anomalies')\n",
        "plt.xlabel('Sample'); plt.ylabel('Reconstruction RMSE')\n",
        "\n",
        "plt.scatter(np.arange(rmse_test.size), rmse_test,\n",
        "            c=list(sample_label), cmap='viridis', alpha=0.5, s=40)\n",
        "\n",
        "plt.scatter(anomalies, rmse_test[anomalies], c='r', s=40)\n",
        "plt.plot([0, rmse_test.size], [threshold, threshold], c='r')\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTjSc3Zg9tMb"
      },
      "source": [
        "# Curva ROC\n",
        "\n",
        "La curva ROC o `Receiver Operating Characteristic` es una representación gráfica de la variación de la sensibilidad de un clasificador binario, respecto a su especificidad, al variar el umbral de discriminación o `threshold`.\n",
        "\n",
        "Recordemos que en un clasificador binario, la sensibilidad corresponde al accuracy de `Verdaderos Positivos (TP)`, mientras que la especificidad corresponde al accuracy de `Verdaderos Negativos (TN)`.\n",
        "\n",
        "$Sensibilidad = \\frac{TP}{TP + FN}$\n",
        "\n",
        "$Especificidad = \\frac{TN}{TN + FP}$\n",
        "\n",
        "Analizar la curva ROC de un clasificador, permite por un lado evaluar la calidad del clasificador, como también seleccionar el umbral apropiado para la aplicación. Recordar que el umbral siempre dependerá del costo particular que tengan los `Falsos Positivos` y los `Falsos Negativos`. A continuación se define una función que genera la curva ROC en base al `autoencoder` que le entreguemos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU-Mca7zB1Jl"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "def generate_ROC(X, Y, autoencoder):\n",
        "  \"\"\"\n",
        "  -> None\n",
        "\n",
        "  genera y gráfica la curva Receiver Operating Characteristic sobre el detector\n",
        "  de anomalías dado por el modelo autoencoder entregado.\n",
        "\n",
        "  :param np.array X:\n",
        "    datos a clasificar mediante el detector de anomalías.\n",
        "  :param np.array Y:\n",
        "    etiquetas reales de los datos X.\n",
        "  :param keras.model autoencoder:\n",
        "    modelo a partir del cual se construye el detector de anomalías.\n",
        "\n",
        "  :returns:\n",
        "    gráfico ROC.\n",
        "  \"\"\"\n",
        "\n",
        "  # obtener indices nominal_idx de samples nominales\n",
        "  # ** esto es específico para este caso de estudio **\n",
        "  RUL = Y_test.flatten() \n",
        "  nominal_idx = np.where( RUL > 0.3 )[0]\n",
        "  degraded_idx = np.where( RUL <= 0.3 )[0]\n",
        "\n",
        "  # segementar samples nominales y degradados\n",
        "  X_nominal = X[nominal_idx, :]\n",
        "  X_degraded = X[degraded_idx, :]\n",
        "\n",
        "  # obtener reconstrucciones mediante el autoencoder\n",
        "  AE_nominal = autoencoder(X_nominal)\n",
        "  AE_degraded = autoencoder(X_degraded)\n",
        "\n",
        "  # obtener rmse de reconstrucciones\n",
        "  rmse_nominal = np.sqrt( np.mean( np.power(X_nominal - AE_nominal, 2 ), axis=1) )\n",
        "  rmse_degraded = np.sqrt( np.mean( np.power(X_degraded - AE_degraded, 2 ), axis=1) )\n",
        "\n",
        "  # ---\n",
        "  # generar curva ROC\n",
        "  min_rmse = np.min( np.hstack([rmse_nominal, rmse_degraded]), axis=None )\n",
        "  max_rmse = np.max( np.hstack([rmse_nominal, rmse_degraded]), axis=None )\n",
        "  threshold = np.linspace(min_rmse, max_rmse, 1000)\n",
        "\n",
        "  S, R = [], []\n",
        "  # para cada umbral en el rango threshold\n",
        "  for t in threshold:\n",
        "    # obtener sensibilidad (True Positives/All Positives)\n",
        "    anomalies = np.where(  )[0]\n",
        "    sensibilidad = \n",
        "\n",
        "    # obtener ratio (False Positives/All Negatives)\n",
        "    anomalies = np.where(  )[0]\n",
        "    ratio = \n",
        "\n",
        "    # registrar valores en listas\n",
        "    S.append(sensibilidad)\n",
        "    R.append(ratio)\n",
        "\n",
        "  # print AUC\n",
        "  area = auc(R, S)\n",
        "  print('AUC: {:1.4f}'.format(area))\n",
        "\n",
        "  # visualizar\n",
        "  plt.figure( figsize=(12, 4) )\n",
        "  plt.plot(R, S)\n",
        "\n",
        "  plt.plot([0, 1], [0, 1], 'r')\n",
        "  plt.xlabel('1 - Especificidad')\n",
        "  plt.ylabel('Sensibilidad')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# generar curva ROC sobre los X_test\n",
        "generate_ROC(X_test, Y_test, VAE_model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}