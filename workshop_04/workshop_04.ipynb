{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_04",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8STlcYRRB7v"
      },
      "source": [
        "# Defective PV Module Cells Transfer Learning\n",
        "\n",
        "En ocasiones, no resulta del todo eficiente entrenar un modelo convolucional desde cero pues es posible que para alcanzar una mejor abstracción en la extracción de features dentro del modelo se requieran una mayor cantidad de datos e incluso un mayor tiempo de entrenamiento, lo que no siempre será factible. Una forma de baipasear este problema es reutilizar las arquitecturas y parámetros de modelos previamente entrenados para la resolución de otros problemas similares al de interés.\n",
        "\n",
        "En este sentido, el transfer learning consiste en adaptar modelos previamente desarrollados para facilitar el aprendizaje del nuevo modelo sobre el problema de interés, del mismo modo en que alguien que sabe guitarra podría aprender a tocar bajo más rapidamente que alguien sin ninguna experiencia musical.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_04/bin/banner.png\" width=\"950\">\n",
        "\n",
        "\n",
        "Convenientemente, `keras` provee de una serie de modelos pre-entrenados de alto nivel mediante su módulo `applications`, con funcionalidades tanto para cargar las arquitecturas y parámetros, como para adaptarlo a las estructuras y datos del problema en estudio. Así, en este workshop volveremos a usar el dataset de electroluminiscencias de celdas solares fotovoltaicas para el desarrollo de un modelo convolucional clasificación mediante transfer learning.\n",
        "\n",
        "- https://keras.io/api/applications/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX4uqHiVh6oL"
      },
      "source": [
        "## ELPV Dataset\n",
        "\n",
        "Para recordar el contenido del dataset que utilizaremos, este cuenta con `2624` imágenes de `300x300 px`, en escala de grises, con celdas fotovoltacias bajo distintos niveles de degradación. De este modo, cada imagen o muestra cuenta con una etiqueta que indica la probabilidad, estimada bajo la evaluación de un experto, de que la muestra presente un estado defectuoso.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_03/bin/banner.png\" width=\"950\">\n",
        "\n",
        "El dataset se encuentra publicado en un repositorio github:\n",
        "- https://github.com/zae-bayern/elpv-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QRuGMdAipQk",
        "outputId": "01d2f0c4-2360-49ca-e03e-ee7c7b20dd26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# cargar GitHub https://github.com/zae-bayern/elpv-dataset.git\n",
        "!git clone https://github.com/zae-bayern/elpv-dataset.git\n",
        "%cd /content/elpv-dataset/utils\n",
        "\n",
        "from elpv_reader import load_dataset\n",
        "\n",
        "# importar colección de imágenes y etiquetas\n",
        "images, proba, types = load_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'elpv-dataset'...\n",
            "remote: Enumerating objects: 2686, done.\u001b[K\n",
            "remote: Total 2686 (delta 0), reused 0 (delta 0), pack-reused 2686\u001b[K\n",
            "Receiving objects: 100% (2686/2686), 90.79 MiB | 49.01 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "/content/elpv-dataset/utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvVr_KcnAmZy"
      },
      "source": [
        "También aprovecharemos de cargar el GitHub del curso deeplearningfallas para disponer de algunas utilidades que nos serán de utilidad más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufu7_fATAzgz",
        "outputId": "34d55d83-063e-41a0-ffde-198624f64875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# cargar GitHub https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/\n",
        "!git clone https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/deeplearningfallas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'deeplearningfallas'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 321 (delta 26), reused 0 (delta 0), pack-reused 249\u001b[K\n",
            "Receiving objects: 100% (321/321), 36.91 MiB | 22.22 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "/content/deeplearningfallas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGYl8kJejURi"
      },
      "source": [
        "---\n",
        "# Training Data\n",
        "\n",
        "Vale la pena recordar que en este dataset las probabilidades en `proba` no siguen una distribución continua entre `[0.0, 1.0]`. En cambio, este valor depende de la incertidumbre del experto al realizar el diagnóstico de daño, como se muestra en la tabla a continuación.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_03/bin/data_table.png\" height=\"140\">\n",
        "\n",
        "Así, de la misma manera que en el workshop anterior, agruparemos estas cuatro clases en solamente dos: `functional cells` y `defective cells`. Por un lado, la clase `functional cells` estará compuesto únicamente de las muestras con probabilidad `0.0`, mientras que `defective cells` agrupará el resto de las clases en el dataset. De este modo, aparte de aislar completamente las celdas que con certeza no presentan ninguna clase de defectos, dado que la función `softmax` indica la probabilidad de pertenencia a cada una de las clases, podemos utilizar esta propiedad para comparar las predicciones del modelo con las etiquetas iniciales.\n",
        "\n",
        "Por otro lado, volveremos a utilizar la función `train_test_split` de `sklearn` para dividir los datos en los sets de entrenamiento `(X_train, Y_train)` y testing `(X_test, Y_test)`. Esta función dividirá de manera aleatoria los datasets ingresados en dos subonjuntos o `subsets` de entrenamiento y testing.\n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljyRymI6Q3r6",
        "outputId": "adcbd0ee-dc19-4cf2-e742-3546ba635e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# transformar lista de probabilidades\n",
        "labels = np.array(  , dtype=int) \n",
        "\n",
        "# transformar a one hot encoding\n",
        "Y = to_categorical(  )\n",
        "\n",
        "# ---\n",
        "# transformar images a shape (samples, height, width, channel)\n",
        "# compatible para el entrenamiento.\n",
        "# adicionalmente transformaremos las imágenes a RGB para compatibilizar\n",
        "# nuestros datos con el modelos que usaremos para el transfer learning.\n",
        "# una manera rápida de hacer este proceso es mediante np.stack\n",
        "X = np.reshape( images, (-1, 300, 300) )\n",
        "X = np.stack( [X, X, X], axis=3 )\n",
        "\n",
        "# realizar data splitting\n",
        "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=, random_state= )\n",
        "\n",
        "# print sample distribution\n",
        "print( 'train split: {:d} samples'.format(X_train.shape[0]) )\n",
        "print( '\\ntesting split: {:d} samples'.format(X_test.shape[0]) )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train split: 2099 samples\n",
            "\n",
            "testing split: 525 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2evSZUEWuW-b"
      },
      "source": [
        "---\n",
        "## Transfer Learning\n",
        "\n",
        "El VGG-16 es un modelo convolucional propuesto por K. Simonyan y A. Zisserman, del Visual Graphics Group (VGG) de la Universidad de Oxford y fue descrito en el paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition (2014)\". Este modelo fue desarrollado sobre el ImageNet Large Scale Visual Recognition Challenge, también referido simplemente como ImageNet, el cual consiste en un dataset de más de 14 millones de imágenes para la clasificación de alrededor de 1000 de clases diferentes de objetos. En 2014, la red VGG-16 fue propuesta al challenge alcanzando un 92.7% de accuracy.\n",
        "\n",
        "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=\"600\">\n",
        "\n",
        "Para este caso particular, utilizaremos la VGG-16 como un extractor de features, donde usaremos el output de la última capa convolucional del modelo como input para nuestro modelo de clasificación de daño. Recordemos que en general, las capas convolucionales cercanas a la entrada de la red aprenden a extraer low-level features de la imagen, mientras que aquellas más cercanas a la salida suelen aprender features con una mayor abstracción y complejidad para la interpretación final de la imagen de entrada.\n",
        "\n",
        "De este modo, cargaremos el modelo VGG-16 contenido en `keras.applications` y acoplaremos la sección convolucional del modelo con una serie de capas `Dense` para la clasificación binaria del problema.\n",
        "\n",
        "- https://keras.io/api/applications/vgg/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIXsDck8uWBa"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.models import Model\n",
        "\n",
        "# ---\n",
        "# inicializar el modelo VGG-16\n",
        "# el parámetro include_top permite especificar si se cargarán las capas\n",
        "# fully connected del modelo o bien, únicamente la sección convolucional.\n",
        "# el parámetro weights permite definir si se cargarán los pesos pre-entrenados\n",
        "# del modelo o si estos se inicializarán de manera aleatoria.\n",
        "# https://keras.io/api/applications/vgg/\n",
        "\n",
        "input_shape = \n",
        "VGG = VGG16(include_top= , weights= , input_shape=  )\n",
        "\n",
        "# ---\n",
        "# configurar modelo VGG-16 para la extracción de features\n",
        "\n",
        "# extraer input del modelo VGG-16\n",
        "# tanto las keras.layers como los keras.models poseen el atributo input\n",
        "input = \n",
        "\n",
        "# extraer output de la última capa del modelo VGG-16\n",
        "# tanto las keras.layers como los keras.models poseen el atributo output\n",
        "VGG_output = \n",
        "\n",
        "# generar sección VGG-16 y detener el entrenamiento de sus parámetros\n",
        "VGG_model = Model(  ,  )\n",
        "\n",
        "# las keras.layers y los keras.models poseen el atributo trainable (bool).\n",
        "# este permite \"congelar\" los parámetros de la capa o modelo, de modo que\n",
        "# los pesos no serán ajustados durante el entrenamiento.\n",
        "for layer in VGG_model.layers:\n",
        "  layer.trainable = \n",
        "\n",
        "# print VGG_model.summary()\n",
        "VGG_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oplmiUMK-ncm"
      },
      "source": [
        "Así hemos configurado la primera sección convolucional de nuestro modelo de clasificación, el cual extraerá los `feature map` aprendidos para el reconocimiento de objetos dentro del problema de ImageNet.\n",
        "\n",
        "Resulta interesante observar el resultado preliminar de estos `feature maps` al ingresar una imagen del ELPV Dataset. Para esto ultilizaremos la función `plot_img_samples` que usamos en el workshop anterior y que se encuentra en el módulo `utils` del github del curso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKy3D-svAVtk"
      },
      "source": [
        "from utils import plot_img_samples\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# obtener imagen de muestra del ELPV dataset\n",
        "# en este caso utilizaremos el dato número 288\n",
        "img = X_train[288, :, :, :]\n",
        "\n",
        "# visualizar imagen de muestra\n",
        "plt.figure( figsize=(5, 5) )\n",
        "plt.imshow(img)\n",
        "\n",
        "# ---\n",
        "# extraer VGG-16 feature maps\n",
        "x = np.reshape(img, (1, 300, 300, 3))\n",
        "VGG16_fmap = \n",
        "\n",
        "# reordenar feature maps a (feature_maps, height, width)\n",
        "fmaps = np.zeros(  )\n",
        "for i in range( 512 ):\n",
        "  fmap = VGG16_fmap[:, :, :, i]\n",
        "  fmaps[i, :, :] = np.reshape( fmap, (1, 9, 9) )\n",
        "\n",
        "# visulizar fmaps mediante plot_img_samples\n",
        "plot_img_samples(fmaps, range(50), grid=(5, 10),\n",
        "                 figsize=(15,15),title='VGG-16 feature maps')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8tCJua9sdQv"
      },
      "source": [
        "Ahora, como se mencionó anteriormente, finalizaremos nuestro modelo de clasificación agregando una serie de capas `Dense`, para terminar de procesar la información, hasta una última capa de salida `softmax`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vGjL3HCtcCF"
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas Dense para procesar la\n",
        "# información hasta la capa de salida.\n",
        "# https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "model = Flatten()(VGG_output)\n",
        "model = Dropout(rate= )(model)\n",
        "model = Dense(units= , activation= )(model)\n",
        "\n",
        "model = Dropout(rate= )(model)\n",
        "model = Dense(units= , activation= )(model)\n",
        "\n",
        "# ---\n",
        "# por último debemos configurar nuestra capa de salida\n",
        "# dado que el modelo consiste en uno de clasificación emplearemos\n",
        "# la función softmax, donde cada nodo indicará la probabilidad de que\n",
        "# los datos correspondan a una de las etiquetas o estados de salud.\n",
        "labels_num = 2\n",
        "model_output = Dense(units=labels_num, activation=  )(model)\n",
        "\n",
        "# ---\n",
        "# generar modelo de clasificación\n",
        "ELPV_Model = Model(  ,  )\n",
        "\n",
        "# print ELPV_Model.summary()\n",
        "ELPV_Model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B5WmSP285HO"
      },
      "source": [
        "## Compile Model\n",
        "\n",
        "Antes de poner a entrenar al modelo, es necesario realizar unas configuraciones adicionales. En particular, debemos especificar la función de pérdida o `loss function` que se optimizará durante el entrenamiento y el método de optimización como SGD o Adam.\n",
        "- https://keras.io/api/models/model_training_apis/\n",
        "- https://keras.io/api/optimizers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPaQ1gw_87iO"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# configurar optimizador Adam\n",
        "# https://keras.io/api/optimizers/adam/\n",
        "opt = Adam( learning_rate=1e-3 )\n",
        "\n",
        "# ---\n",
        "# compilar modelo siguiendo como función de pérdida\n",
        "# la categorical crossentropy\n",
        "ELPV_Model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjKYqaay9DZd"
      },
      "source": [
        "## Model Training and Callbacks\n",
        "Hemos llegado a la parte final del proceso, para entrenar nuestro modelo debemos especificar los sets que utilizaremos para el proceso `(X_train, Y_train)`, la cantidad de `epochs` que durará el entrenamiento, y el `batch size` de muestras que se irán entregando al modelo a medida que este va iterativamente ajustando sus parámetros.\n",
        "\n",
        "Ahora, dado que el entrenamiento de los modelos CNN suele demorar bastante más tiempo que un modelo Fully Connected como el del workshop anterior, suele convenir utilizar `Callbacks` durante el entrenamiento. Los `Callbacks` consisten en métodos que realizan una serie de acciones a medida que el entrenamiento se lleva a cabo. Por supuesto, `Keras` ya cuenta con una selección de `Callbacks` predefinidos listos para utilizar, como `EarlyStopping` y `ModelCheckpoint`.\n",
        "\n",
        "- https://keras.io/api/callbacks/\n",
        "\n",
        "En este caso, utilizaremos `ModelCheckpoint`, el cual se encargará de monitorear el `val_acc` durante el entrenamiento y guardar el modelo cada vez que este alcance un nuevo máximo. De este modo, si en algún punto llegase a ocurrir `overfitting`, se tendrá un respaldo del modelo que podrá ser importado posteriormente.\n",
        "\n",
        "- https://keras.io/api/callbacks/model_checkpoint/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt6KQTzN9CrD"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from utils import plot_loss_function\n",
        "\n",
        "# ubicación en donde se guardará el modelo\n",
        "save_path = '//content//model_checkpoint.h5'\n",
        "\n",
        "# inicializar ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(save_path, monitor='val_accuracy',\n",
        "                             save_best_only=True)\n",
        "\n",
        "# realizar rutina de entrenamiento\n",
        "model_history = ELPV_Model.fit(X_train, Y_train,\n",
        "                               batch_size= , epochs= ,\n",
        "                               validation_data= ,\n",
        "                               callbacks=  )\n",
        "\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(model_history, figsize=(10,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7px2BjLQCiK"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Finalmente, una vez entrenado nuestro modelo debemos evaluar su desempeño. Nuevamente dada la poca cantidad de datos, utilizaremos los datos de validación como datos de testing. Para utilizar el `keras.Model` sobre nuevos datos de clasificación, conviene utilizar el método `keras.Sequential.predict`.\n",
        "\n",
        "Por otro lado, para cargar el modelo guardado por el `ModelCheckpoint` durante el entrenamiento, se puede utilizar la función `keras.models.load_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0r3DdbQFp0"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "from utils import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# cargar modelo\n",
        "model_path = '//content//model_checkpoint.h5'\n",
        "ELPV_Model = load_model(model_path)\n",
        "\n",
        "# obtener predicciones de X_test\n",
        "Y_pred = ELPV_Model.predict(  )\n",
        "\n",
        "# para que el resultado nos sea más intuitivo transformaremos\n",
        "# las etiquetas nuevamente a non one-hot-encoding\n",
        "# utilizando np.argmax\n",
        "labels_pred = np.argmax( , axis=1 )\n",
        "labels_true = np.argmax( , axis=1 )\n",
        "\n",
        "# calcular accuracy de la clasificación.\n",
        "accuracy = accuracy_score(labels_true, labels_pred)\n",
        "print('testing accuracy: {:1.3f}'.format(accuracy))\n",
        "\n",
        "# plot de matriz de confusión\n",
        "plot_confusion_matrix(labels_true, labels_pred, ['Defective', 'Functional'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}