{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw6EXItut1vF"
      },
      "source": [
        "# C-MAPSS Aircraft Engine Simulator Dataset\n",
        "\n",
        "En los últimos años, dentro del campo de la Gestión de Activos Físicos, diversas investigaciones han propuesto la utilización de modelos de deep learning cada vez más sofisticados para la estimación y predicción del tiempo de vida remanente (RUL) de equipos y/o componentes particulares. El tiempo de vida remanente o Remaining Useful Life (RUL) consiste en una variable utilizada principalmente para describir la degradación temporal de un activo durante su operación y describe el tiempo de operación remanente del activo antes de que ocurra su falla. De este modo, mejores estimadores de esta variable resultan de suma importancia a la hora de gestionar e implementar mejores políticas de mantenimiento y operación.\n",
        "\n",
        "En este workshop implementaremos un modelo recurrente mediante `tensorflow` y `keras` para la predicción del RUL de un turbofán de aviación, a partir de la serie temporal de mediciones de sus sensores.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_05/bin/nasa_logo.png\" height=\"200\"><img src=\"https://www.researchgate.net/profile/Xiaolei_Fang/publication/310789705/figure/fig2/AS:462413100261377@1487259282977/Simplified-diagram-of-engine-simulated-in-C-MAPSS-6.png\" height=\"200\">\n",
        "\n",
        "El Comercial Modular Aero-Propulsion System Simulation (C-MAPSS) es un software desarrollado por NASA como ambiente de simulación de motores de reacción tipo turbofán. Así, esta herramienta permite la implementación y evaluación de algoritmos de control y diagnóstico sobre la operación de un motor turbofán de 90.000 lbf de propulsión.\n",
        "\n",
        "En particular, el C-MAPSS dataset que utilizaremos en esta ocación consiste en múltiples simulaciones de degradación de motores turbofán bajo distintas condiciones, tanto mecánicas como operacionales. En este sentido, el dataset se compone de más de `27000` series temporales de los sensores del turbofán que, gracias a la simulación de este, han podido ser asociadas al RUL del motor.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-R8SDBLGuBS"
      },
      "source": [
        "## C-MAPSS Dataset\n",
        "\n",
        "En esta sección nos encargaremos de cargar y explorar el dataset que utilizaremos, denominado `C-MAPSS FD001`, que contiene `100` simulaciones de motores mecánicamente distintos bajo condiciones nominales de operación. Nuevamente, para facilitar la carga del dataset a este entorno de Google Colab el achivo `CMAPSS_FD001.npz` ha sido cargado a un Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoNStL9XtzYl",
        "outputId": "2583f148-08c2-4a08-ff34-e47a63a766bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# inicializar GoogleDrive con credenciales de autorización\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# crear carpeta para descargar los archivos .npz\n",
        "!mkdir /content/datasets\n",
        "\n",
        "# Google Drive IDs para descargar los archivos .npz\n",
        "files_id = [('CMAPSS_FD001.npz', '1DNJFrQKB4I7SqjpFmM5SsKRaN7O9XCNZ')]\n",
        "\n",
        "# comenzar descarga\n",
        "print('descargando datasets: ', end='')\n",
        "\n",
        "for filename, id in files_id:\n",
        "  save_path = os.path.join('/content/datasets', filename)\n",
        "\n",
        "  # descargar y guardar en /content/datasets\n",
        "  downloaded = drive.CreateFile({'id': id}) \n",
        "  downloaded.GetContentFile(save_path)\n",
        "\n",
        "# indicar descarga terminada\n",
        "print('done')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "descargando datasets: done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DYsx7d0J7W5"
      },
      "source": [
        "Carguemos este archivo mediante `np.load()` y exploremos las estructuras y datos que contiene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyyrU9ekKkR5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# cargar archivo CMAPSS_FD001.npz\n",
        "dataset = np.load('/content/datasets/CMAPSS_FD001.npz', allow_pickle=True)\n",
        "\n",
        "# print keys del dataset\n",
        "print(  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3HlYqCndIHq"
      },
      "source": [
        "También, como ya es rutinario, aprovecharemos de cargar el GitHub del curso deeplearningfallas para disponer de algunas utilidades que nos serán de utilidad más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8PvO5qedOiq",
        "outputId": "2a6f125f-47ee-46d5-c88a-c37a5a362e64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# cargar GitHub https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/\n",
        "!git clone https://github.com/cherrerab/deeplearningfallas.git\n",
        "%cd /content/deeplearningfallas"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'deeplearningfallas'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 360 (delta 43), reused 0 (delta 0), pack-reused 249\u001b[K\n",
            "Receiving objects: 100% (360/360), 37.16 MiB | 20.12 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n",
            "/content/deeplearningfallas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmfqu8zbLAm5"
      },
      "source": [
        "---\n",
        "# Training Data\n",
        "\n",
        "Podemos ver que el dataset ha sido previamente separado en conjuntos de `training` y `testing`. En particular, de acuerdo a la descripción de este dataset, cada uno de estos conjuntos están separados en `100` simulaciones de turbinas distintas. Luego, cada una de estas simulaciones contiene varias muestras de series temporales registradas durante la operación que están asociadas a un valor RUL en el conjunto `Y_train` o `Y_test`.\n",
        "\n",
        "Para entender mejor esto, veamos un ejemplo a continuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pWeElexP1WD",
        "outputId": "ab42eaf8-2157-45a0-9a76-f83f1ef21175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# extraer conjuntos de training y testing del dataset\n",
        "X_train, Y_train = dataset['x_train'], dataset['y_train']\n",
        "X_test, Y_test = dataset['x_test'], dataset['y_test']\n",
        "\n",
        "# print dimensiones de los conjuntos\n",
        "print('X_train.shape: ', X_train.shape)\n",
        "print('Y_train.shape: ', Y_train.shape)\n",
        "print('\\nX_test.shape: ', X_test.shape)\n",
        "print('Y_test.shape: ', Y_test.shape)\n",
        "\n",
        "# --\n",
        "# podemos ver que cada uno de los conjuntos posee una forma (100,)\n",
        "# donde cada uno de estos elementos corresponde a una\n",
        "# simulación de CMAPSS distinta.\n",
        "\n",
        "# ahora podemos extraer la series temporales de una de las\n",
        "# simulaciones de la siguiente forma.\n",
        "X_50 = X_train[50]\n",
        "Y_50 = Y_train[50]\n",
        "\n",
        "# print dimensiones de los datos de esta simulación\n",
        "print('\\nX_train[50].shape: ', X_50.shape)\n",
        "print('Y_train[50].shape: ', Y_50.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape:  (100,)\n",
            "Y_train.shape:  (100,)\n",
            "\n",
            "X_test.shape:  (100,)\n",
            "Y_test.shape:  (100,)\n",
            "\n",
            "X_train[50].shape:  (184, 30, 14, 1)\n",
            "Y_train[50].shape:  (184, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Y-THuRT_T_"
      },
      "source": [
        "De acuerdo a la descripción del dataset, y como se puede notar del resultado anterior, los datos de cada simulación en el conjunto `X_train` son de la forma `(n_samples, n_timesteps, n_features, 1)`. Donde en este caso `n_features` corresponde a la cantidad de sensores dentro de la simulación (`14`), mientras `n_timesteps` corresponde a la cantidad de puntos temporales o mediciones dentro de la serie (`30`). Por otro lado, los datos del conjunto `Y_train` son de la forma `(n_samples, 1)`, donde cada valor corresponde al RUL de las respectivas series temporales en el conjunto `X_train`.\n",
        "\n",
        "Ahora, para generar los conjuntos `training` y `testing` que finalmente utilizaremos para desarrollar nuestros modelos, debemos concatenar las series de todas las simulaciones de `C-MAPSS` y modificar sus dimensiones para que sean compatibles con las capas y estructuras de `tensorflow` y `keras`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NjsI-FAVtU8"
      },
      "source": [
        "# concatenar y reconfigurar datos de las 100 simulaciones CMAPSS\n",
        "\n",
        "# los datos X deben ser de la forma (n_samples, n_timesteps, n_features)\n",
        "X_train = \n",
        "X_train = \n",
        "\n",
        "X_test = \n",
        "X_test = \n",
        "\n",
        "# los datos Y deben ser de la forma (n_samples, 1)\n",
        "Y_train = \n",
        "Y_train = \n",
        "\n",
        "Y_test = \n",
        "Y_test = \n",
        "\n",
        "# print dimensiones de los nuevos conjuntos\n",
        "print('X_train.shape: ', X_train.shape)\n",
        "print('Y_train.shape: ', Y_train.shape)\n",
        "print('\\nX_test.shape: ', X_test.shape)\n",
        "print('Y_test.shape: ', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo1_Q6W0VZ1l"
      },
      "source": [
        "Como tenemos una gran cantidad de datos tanto en el conjunto de `training`, como en el de `testing`, siguiendo el procedimiento apropiado para el desarrollo de modelos de deep learning, utilizaremos la función `sklearn.train_test_split` para generar un conjunto de `validation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68RMVgNAmyLo",
        "outputId": "8fac4700-615a-4c88-8f86-9d7da210d718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# realizar data splitting\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,\n",
        "                                                  train_size= , test_size= ,\n",
        "                                                  random_state= )\n",
        "\n",
        "# print sample distribution\n",
        "print( 'train split: {:d} samples'.format(X_train.shape[0]) )\n",
        "print( '\\nvalidation split: {:d} samples'.format(X_val.shape[0]) )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train split: 8865 samples\n",
            "\n",
            "validation split: 3547 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7LqlLeaZ1Ct"
      },
      "source": [
        "Finalmente, para finalizar la exploración y procesamiento de los datos, podemos visualizar la información de una de las series temporales, separandola en cada uno de los sensores, para tener una mejor noción de los datos que estamos utilizando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQXktA_VbKr9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# obtener serie temporal del conjunto train a visualizar\n",
        "ts_24 = X_train[24, :, :]\n",
        "\n",
        "# obtener RUL correspondiente\n",
        "RUL = Y_train[24, 0]\n",
        "\n",
        "# visualizar datos de los sensores\n",
        "fig = plt.figure(figsize=(12, 5))\n",
        "\n",
        "# para cada uno de los 14 sensores de la serie temporal\n",
        "for s in range(14):\n",
        "  sensor_ts = ts_24[:, s]\n",
        "  # plotar respecto al tiempo\n",
        "  legend = 'Sensor {:02d}'.format(s)\n",
        "  plt.plot(np.arange(30), sensor_ts, label=legend)\n",
        "\n",
        "# configurar resto del gráfico\n",
        "plt.title('RUL = {:1.3f}'.format(RUL))\n",
        "plt.ylabel('Sensor Output')\n",
        "plt.xlabel('Timestep')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFNvCKLNbKMl"
      },
      "source": [
        "---\n",
        "# Model Building\n",
        "\n",
        "Como se detalló anteriormente, el objetivo de este workshop es implementar un modelo recurrente que sea capaz de estimar o bien, predecir el tiempo de vida remanente (RUL) de la turbina a partir del registro temporal de sus sensores. Es decir, el modelo debe procesar secuencialmente la información de cada uno de los sensores para producir la estimación del RUL correspondiente.\n",
        "\n",
        "Para configurar nuestro modelo de regresión utilizaremos nuevamente la librería `keras` o `tf.keras`. Keras es una API de alto nivel para la creación y el entrenamiento de modelos de deep learning. Está orientada y diseñada para la construcción de modelos de forma modular o en bloques. De este modo, ofrece un framework mucho más amigable e intuitivo para principiantes, a la vez que mantiene un estructura personalizable y versátil que permite a usuarios más avanzados incorporar nuevas ideas.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_02/bin/keras_logo.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfrmaXKf2DH"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "Dado que nuestros datos consisten en series temporales de distintas `features`, debemos ocupar arquitecturas o bien, capas recurrentes para procesar estos registros sin perder la secuencialidad de la información.\n",
        "\n",
        "Convenientemente, `keras` provee de distintas implementaciones de capas recurrentes listas para utilizar, tales como la `keras.layers.RNN` y la `keras.layers.LSTM`.\n",
        "\n",
        "- https://keras.io/api/layers/recurrent_layers/\n",
        "\n",
        "De esta forma, en términos generales, compondremos nuestros modelos de deep learning de una serie de capas recurrentes, que se encargarán de procesar paralelamente la información secuencial de cada uno de los sensores del turbofán, para luego dar paso a una capa `keras.layers.Flatten` que combinará la información extraída de las secuencias.\n",
        "\n",
        "Finalmente, una serie de capas `keras.layers.Dense` se encargarán de procesar los patrones y features extraídas para dar paso a un última capa `Dense` con activación `linear` que generará la estimación del RUL de la turbina."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GScDB0L5ixqX"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "\n",
        "from keras.layers import SimpleRNN as RNN\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# inicializar modelo keras.Sequential\n",
        "model = Sequential()\n",
        "\n",
        "# ---\n",
        "# primero debemos agregar nuestra capa Input donde debemos especificar\n",
        "# las dimensiones de los datos que se ingresarán al modelo\n",
        "# las capas recurrentes reciben tensores de la forma (n_timesteps, n_features)\n",
        "input_dim = ( 30, 14 )\n",
        "model.add( Input( shape=input_dim ) )\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas RNN o LSTM.\n",
        "\n",
        "# similar a la capas Dense, las keras.layers.SimpleRNN y keras.layers.LSTM\n",
        "# reciben la cantidad de unidades o nodos dentro de la capa y\n",
        "# función de activación con que operarán.\n",
        "\n",
        "# mediante el parámetro return_sequences es posible definir si la capa\n",
        "# retornará la secuencia de estados latentes de la capa o unicamente\n",
        "# la salida o valor final de la secuencia.\n",
        "# https://keras.io/api/layers/recurrent_layers/simple_rnn/\n",
        "# https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "\n",
        "\n",
        "model.add( LSTM(units=64, activation='relu', return_sequences=True ) )\n",
        "model.add(  )\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas Dense para procesar la\n",
        "# información hasta la capa de salida.\n",
        "# https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "model.add( Flatten() )\n",
        "\n",
        "model.add( Dense(units=96, activation='relu') )\n",
        "model.add(  )\n",
        "\n",
        "# ---\n",
        "# por último debemos configurar nuestra capa de salida\n",
        "# dado que el modelo consiste en uno de regresión emplearemos una\n",
        "# capa Dense con función de activación linear\n",
        "model.add( Dense(units=1, activation='linear') )\n",
        "\n",
        "# print model.summary()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x50vOo4ulG6h"
      },
      "source": [
        "## Compile Model\n",
        "\n",
        "Antes de poner a entrenar al modelo, es necesario realizar unas configuraciones adicionales. En particular, debemos especificar la función de pérdida o `loss function` que se optimizará durante el entrenamiento y el método de optimización como SGD o Adam.\n",
        "- https://keras.io/api/models/model_training_apis/\n",
        "- https://keras.io/api/optimizers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu0VML-llJIw"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# configurar optimizador Adam\n",
        "# https://keras.io/api/optimizers/adam/\n",
        "opt = Adam( learning_rate=1e-3 )\n",
        "\n",
        "# ---\n",
        "# compilar modelo siguiendo como función de pérdida\n",
        "# el error cuadrádo medio (mse)\n",
        "model.compile(loss=, optimizer=opt, metrics=[  ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUlBKDlXlZY3"
      },
      "source": [
        "## Model Training\n",
        "Hemos llegado a la parte final del proceso, para entrenar nuestro modelo debemos especificar los sets que utilizaremos para el proceso `(X_train, Y_train)`, la cantidad de `epochs` que durará el entrenamiento, y el `batch size` de muestras que se irán entregando al modelo a medida que este va iterativamente ajustando sus parámetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjib53pTldpq"
      },
      "source": [
        "from utils import plot_loss_function\n",
        "\n",
        "# realizar rutina de entrenamiento\n",
        "model_history = model.fit(X_train, Y_train,\n",
        "                          batch_size=, epochs=50,\n",
        "                          validation_data=(X_val, Y_val))\n",
        "\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(model_history, figsize=(10,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XM2Z9roeFiw"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Como siempre, una vez entrenado nuestro modelo debemos evaluar su desempeño. Dado que ahora disponemos de una gran cantidad de datos, podemos evaluar correctamente el modelo frente a un conjunto `testing` que no ha visto antes. Para obtener los estimados/predicciones del RUL sobre nuevas series temporales, conviene utilizar el método `keras.Sequential.predict`, análogamente a como hemos hecho en las arquitecturas anteriores.\n",
        "\n",
        "Por otro lado, dado que estamos frente a un problema de regresión no podemos utilizar las herramientas de visualización anteriores como la matriz de consfusión. En este caso utilizaremos un `scatter plot` medainte la función `utils.plot_predict_scatter` para visualizar la correlación entre las predicciones `Y_pred` y los valores reales `Y_true`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z4SlI92fXcO"
      },
      "source": [
        "from utils import plot_predict_scatter\n",
        "\n",
        "# obtener predicciones de X_test del modelo\n",
        "Y_pred = model.predict(  )\n",
        "\n",
        "# obtener valores RUL reales del X_test\n",
        "Y_true = \n",
        "\n",
        "# calcular el rmse de las predicciones.\n",
        "rmse = \n",
        "print('testing rmse: {:1.3f}'.format(rmse))\n",
        "\n",
        "# plot de matriz de confusión\n",
        "plot_predict_scatter(Y_true, Y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}